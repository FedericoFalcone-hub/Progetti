\documentclass{article}
\usepackage[a4paper, top=2cm, bottom=2cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[dvipsnames]{xcolor}
\title{Intelligenza Artificiale I}
\author{Federico Falcone}
\date{\today}
\begin{document}
\maketitle
\section{Agenti}
\subsection{Agenti e ambienti}
Un \textbf{agente} è qualsiasi cosa possa essere vista come un sistema che percepisce il suo \textbf{ambiente} attraverso i \textbf{sensori} e agisce su di esso mediante \textbf{attuatori}.
Usiamo il termine \textbf{percezione} per indicare i dati che i sensori di un agente percepiscono. La \textbf{sequenzza percettiva} di un agente è la storia completa di tutto ciò che esso ha percepito nella sua esistenza. In generale, \textit{la scelta dell'azione di un agente in un qualsiasi istante può dipendere dalla conoscenscena integrata in esso e dall'intera sequenza percettiva osservata fino a quel punto, ma non da qualcosa che l'agente non abbia percepito}. Il comportamento di un agente quindi è descritto dalla \textbf{funzione agente}, che descrive la corrispondenza tra una qualsiasi sequenza percettiva e una specifica azione. \\
Possiamo immaginare di rappresentare in forma di \textbf{tabella}  la funzione agente che descrive un certo agente. La tabella è una descrizione \textbf{esterna} dell'agente. \textbf{Internamente}, la funzione agente di un agente artificiale sarà implementata da un \textbf{programma agente}, il quale è l'implementazione della funzione agente.

\subsection{Comportarsi correttamente: il concetto di razionalità}
Un \textbf{aagente razionale} è un agente che fa la cosa giusta.
\subsubsection{Misure di prestazione}
Valutiamo il comportamento di un agente considerandone le \textit{conseguenze}. Ciò si chiama \textbf{consequenzialismo}. Quando un agente viene inserito in un ambiente, genera una sequenza di azioni in base alle percezioni che riceve. Questa sequenza di azioni porta l'ambiente ad attraversare una sequenza di \textbf{stati}: se tale sequenza è desiderabile, significa che l'agente si è comportato bene. Questa nozione di desiderabilità è catturata da una \textbf{misura di prestazione} che valuta una sequenza di stati dell'ambiente.

\subsubsection{Razionalità}
In un dato momento, ciò che è razionale dipende da quattro fattori:
\begin{itemize}
    \item la misura di presazione che definisce il criterio di successo;
    \item la conoscenza pregressa del'ambiente da parte dell'agente;
    \item le azioni che l'agente può effettuare;
    \item la sequenza percettiva dell'agente fino all'istante corrente.
\end{itemize}
Questo porta alla definizione di \textbf{agente razionale}: Per ogni possibile sequenza di percezioni, un agente razionale dovrebbe scegliere un'azione che massimizzi il valore ateso della sua misura di prestazione, date le informazioni fornite dalla sequenza percettiva e da ogni ulteriore conoscenza dell'agente.
\subsubsection{Onniscenza, apprendimento e autonomia}
Un agente \textbf{onniscente} conosce il risultato \textit{effettivo} delle sue azioni e può agire di conseguenza. \\ Intraprendere azioni mirate a modificare le percezioni future, chiamato \textbf{information gathering} è una perte importante della razionalità. Un esempio è formato dall'\textbf{esplorazione}. \\ Un agente razionale non si deve limitare solo a raccogliere informazioni, ma deve essere anche in grado id \textbf{apprendere} il più possibile sulla base delle proprie percezioni. 

\subsection{La natura degli ambienti}
\subsubsection{Proprietà degli ambienti operativi}
Gli ambienti devono essere: \begin{itemize}
    \item \textbf{completamente osservabile/parzialmente osservabile}
    \item \textbf{Agente sigolo/multiagente}: gli ambienti multiagente possono essere \textbf{competitivi} oppure \textbf{cooperativi}.
    \item \textbf{deterministico/non deterministico (stocastico)}: è deterministico quando lo stato successivo dell'ambiente è completamente determinato dallo stato corrente e dall'azione eseguita dall'agente.
    \item \textbf{episodico/sequenziale} episodico significa che l'esperienza dell'agente è divisa in episodi atomici. In ogni episodio l'agente riceve una percezione e poi esegue una singola azione. Ogni episodio non dipende dalle azioni intraprese in quelli precedenti. In quelli sequenziali ogni decisione può influenzare tutte quelle successive.
    \item \textbf{statico/dinamico}: se l'ambiente può cambiare mentre un agente sta decidendo come agire è dinamico.
    \item \textbf{discreto/continuo}: la distinzione si applica allo stato dell'ambiente, al modo in cui è gestito il tempo, alle percezioni e azioni dell'agente.
    \item \textbf{noto/ignoto}: si riferisce allo stato di conoscenza dell'agente delle "leggi fisiche" dell'ambiente stesso.
\end{itemize}
\subsection{La struttura degli agenti}
    Il compito dell'intelligenza artificiale è progettare il \textbf{programma agente} che implementa la funzione agente, che fa corrispondere la percezione alle azioni. Diamo per scontato che questo programma sarà eseguito da un dispositivo computazionale dotato di sensori e attuatori fisici; questa prende il nome di \textbf{architettura agente}: \\
    \textit{agente=architettura + programma}
\subsubsection{Programmi agente}
I programmi agente prendono come input la percezione corrente dei sensori e restituiscono un'azione agli attuatori.

\subsubsection{Agenti reattivi semplici}
Questi agenti scelgono le azioni sula base della percezione \textit{corrente}, ignorando tutta la storia percettiva corrente.
\subsubsection{Agenti reattivi basati su modello}
Il modo più efficace di gestire l'osservabilità parziale, per un agente, è \textit{tener traccia della parte del mondo che non può vedere nell'istnate corrente}. Questo significa che l'agente deve mantenere una sorta di \textbf{stato interno} che dipende dalla storia delle percezioni e che quindi riflette almeno una parte degli aspetti non ossevabili dello stato corrente. \\ Aggiornare l'informazione dello stato interno al passaggio del tempo richiede che il programma agente possieda due tipi di conoscenza. Prima di tutto, deve avere informazioni sull'evoluzione del mondo nel tempo, suddivisibili approssimativamente in due parti: gli effetti delle azioni dell'agente e le modalità di evoluzione del mondo indipendentemente dall'agente. Questa conoscenza sul "funzionamento del mondo", viene chiamata \textbf{modello di transizione} del mondo. \\
In seconod luogo, ci servono informazioni su come lo stato del mondo si rifletta nelle percezioni dell'agente. Questo tipo di conoscenza è chiamato \textbf{modello sensoriale}. 
\\ Il modello di transizione e il modello sensoriale, insieme, consentono a un agente di tenere traccia dello stato del mondo, per quanto possibile date le limitazioni dei sensori. Un agente che utilizza tali modelli prende il nome di \textbf{agesnete basato su modello}.
\subsubsection{Agenti basati su obiettivi}
Conoscere lo stato corrente dell'ambiente non sempre basta e decidere che cosa fare. Oltre che alla descrizione dello stato corrente l'agente ha bisogno di qualche tipo di informazione riguardante il suo \textbf{obiettivo} (goal), che descriva situazioni desiderabili. \\ Talvolta scegliere un'azione in base a un obiettivo è molto semplice, quando questo può essere raggiunto in un solo passo. Altre volte è più difficile. La \textbf{ricerca} e la \textbf{pianificazione} sono sottocampi dell'IA dedicati proprio a identificare le sequenze di azioni che permettono a un agente di raggiungere i propri obiettivi.
\\ Benchè un agente basato su obiettivi sembri meno efficiente, d'altra parte è più \textbf{flessibile}, perchè la conoscenza che guida le sue decisioni è rappresentata esplicitamente e può essere modificata.
\subsubsection{Agenti bassati sull'utilità}
Gli obiettivi forniscono solamente una distinzione binaria tra stati "contenti" e "scontenti", laddove una misura di prestazione più generale dovrebbe permettere di confrontare stati del mondo differenti e misurare precisamente la contentezza che potrebbero portare all'agente. Per descrivere ciò utilizziamo il termine \textbf{utilità}.
Una \textbf{funzione di utilità} di un agente è un'internalizzazione della misura di prestazione. Purchè la funzione di utilità interna e la misura di presazione esterna concordino, un agente che sceglie le zioni per massimizzare l'utilità sarà razionale in base alla misura di prestazione esterna.

\section{Risolvere i problemi con la ricerca}
Quando l'azione giusta da compiere non è subito evidente, un agente uò avere la necessità di \textit{guardare avanti}, cioè considerare una \textbf{sequenza} di azioni che formano un cammino ch eporterà a uno stato obiettivo. Questo tipo di agente è chiamato \textbf{agente risolutore di problemi} e il processo computazionale che effettua è la \textbf{ricerca}. \\ Gli agenti risolutori di problemi utilizzano rappresentazioni \textbf{atomiche} in cui gli stati del mondo sono considerati come entità prive di una struttura interna visibile agli algoritmi per la risoluzione dei problemi. Gli agenti che utilizzano rappresentazioni di stati \textbf{fattorizzate} o \textbf{strutturate} sono solitamente chiamati \textbf{agenti pianificatori}.
\subsection{Agenti risolutori di problemi}
Se l'agente non ha informazioni sufficienti sull'ambiente, ovvero se l'ambiente è \textbf{ignoto}, non può fare altro che eseguire una delle azioni scelte a caso. Con tali informazioni a disposizione, l'agente può eseguire un processo di risoluzione del problema in quattro fasi:
\begin{itemize}
    \item \textbf{Formulazione dell'obiettivo};
    \item \textbf{Formulazione del problema}: l'agente elabora una descrizione degli stati e delle azioni necessarie per raggiungere ll'obiettivo, ovvero un modello astratto della parte del mondo interessata;
    \item \textbf{Ricerca}: prima di effettuare qualsiasi azione nel modno reale, l'agente simula nel suo modello sequenze di azioni, continuando a cercare finchè trova una sequenza che raggiunge l'obiettivo: tale sequenza si chiama \textbf{soluzione};
    \item \textbf{Esecuzione}: l'agente ora può eseguire le azioni specificate nella soluzione, una per volta.
\end{itemize}
\subsubsection{Problemi di ricerca e soluzioni}
Un \textbf{problema} di ricerca può essere definito formalmente come segue:
\begin{itemize}
    \item Un insieme di possibili \textbf{stati} in cui può trovarsi l'ambiente. Lo chiamiamo \textbf{spazio degli stati}.
    \item Lo \textbf{Stato iniziale} in cui si trova l'agente inizialmente.
    \item Un insieme di uno o pià \textbf{stati obiettivo}. A volte è unico, a volte è un piccolo insieme, a volte è definito da una proprietà che è soddisfatta da molti stati.
    \item Le \textbf{azioni} possibili dell'agente. Dato uno stato $s, AZIONI(s)$ restituisce un insieme finito di azioni che possono essere eseguite in $s$. Diciamo che ognuna di questa azioni è \textbf{applicabile} in $s$.
    \[ A(s)=\{a,b,c,...\}\]
    \item Un \textbf{modello di transizione} che descrive ciò che fa ogni azione. Dato uno stato di partenza $s_j$ e un'azione $a \in A(s_i)$, indica uno stato di arrivo $f(s_i,a)$: rappresenta la conseguenza dello svolgere l'azione $a$ nello stato $s$.
    \item Una \textbf{funzione di costo dell'azione}, denotata da $c(s_j,a,f(s_i,a)$, restituisce il costo numerico di applicare l'azione $a$ nello stato $s_i$ per raggiungere lo stato $s'$
    
\end{itemize}
Una sequenza di azioni forma un \textbf{cammino}; una \textbf{soluzione} è un cammino che porta dallo stato iniziale a uno stato obiettivo. Assumiamo che i costi delle azioni siano additivi. Una \textbf{soluzione ottima} è quella che ha il costo minimo. \\ Lo spazio degli stati può essere rappresentato come un \textbf{grafo} in cui i vertici rappresentano gli stati e i collegamenti orientati tra di essi rappresentano le azioni.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Images/ricerca1.png}
\end{figure}

\subsubsection{La formulazione dei problemi}
La formulazione del problema è un \textbf{modello}, ovvero una descrizione matematica astratta.\\ Il processo di rimozione dei dettagli da una rappresentazione prende il nome di \textbf{astrazione} Per una buona formulazione del problema serve il giusto livello di dettaglio.
\\
Come faccio a specificare un problema di search?
\begin{itemize}
    \item \textbf{\textcolor{red}{Approccio esaustivo/esplicito} }: fornire il grafo degli stati in modo completo specificando tutte le transizioni possibili. Il più delle volte questa non è un'opzione percorribile a casua della natura combinatoria dello spazio degli stati.
    \item \textbf{\textcolor{blue}{Approccio implicito}}: possiamo specificare lo stato iniziale e la funzione di transizione in una forma \textbf{compatta}. Il grafo degli stati si "svela" man mano che le azioni vengono valutate.
\end{itemize}
Ci serve una procedura efficiente per ocntrollare se uno stato generato è il goal: \textbf{goal check}.

\subsection{Algoritmi di ricerca}
Un \textbf{algoritmo di ricerca} riceve in input un problema di ricerca e restituisce una soluzione o un'indicazione di fallimento. Considretiamo algoritmi ch esovrappongono un \textbf{albero di ricerca} al grafo dello spazio degli stati, formando vari cammini a partire dallo stato iniziale e carcando di trovarne uno che raggiunga uno stato obiettivo. Ciascun \textbf{nodo} nell'albero di ricerca corrisponde a uno stato nello spazio degli stati e i rami del'albero di ricerca corrispondo ad azioni. La radice dell'alvvero corrisponde allo stato iniziale del problema.
\\ È importante comprendere la distinzione tra spazio degli stati e albero di ricerca. Lo spazio degli stati descrive l'insieme degli stati nel mondo e le azioni che consentono le transizioni da uno stato a un altro. L'albero di ricerca descrive i cammini tra questi stati per raggiungere l'obiettivo. Nell'albero di ricerca possono esserci più cammini per raggiungere qualsiasi stato, ma per ogni nodo dell'albero c'è un cammino univoco per tornare alla radice.

\subsubsection{Obiettivi della ricerca}
Un algoritmo di ricerca \textbf{esplora il grafo delgi stati fin quando non trova la soluzione desiderata}. Nella versione di \textbf{fattibilità} quando viene visitato un nodo di goal viene restituito il percorso che ha portato a quel nodo. Nella versione di \textbf{ottimizzazione} quando viene visitato un nodo di goal, se quasiasi altro possibile percorso per quel nodo ha un costo maggiore, viene restituito il percorso che ha portato a quel nodo. \\ Non basta visitare un nodo di goal, l'algoritmo deve riscostruire il percorso che ha seguito per arrivarci: deve tenere traccia della sua ricerca. Tale traccia può essere mappata su un sotto-grafo di G, detto \textbf{albero} di ricerca.

\subsubsection{Come si valuta un algoritmo di ricerca?}
Possiamo valutare un algoritmo di ricerca lungo diverse dimensioni:
\begin{itemize}
    \item Correttezza
    \item Completezza
    \item Complessità in termini di spazio
    \item Complessità in termini di tempo
\end{itemize}
\subsubsection{Correttezza}
Garanzia che se l'algoritmo restituisce una soluzione, questa è conforme alle caratteristiche specificate nella formulazione del problema. \\ L'algoritmo dice che c'è una soluzione. È vero? E la soluzione che l'algoritmo ha calcolato conduce veramente ad un goal?
\subsubsection{Completezza}
Garanzia che se una soluzione esiste allora l'algoritmo la trova \textbf{sempre}. \\ L'\textit{algoritmo termina sempre? E se dice che non ci sono soluzioni, è vero?} \\
La completezza di solito si dimostra facendo vedere che la ricerca nello spazio degli stati + in grado di visitare tutti gli stati possibili, ap atto di concedere un temrpo arbitrariamente lungo.
\\ \textbf{Se lo spazio degli stati è infinito?} Possiamo chiederci se la riceca è \textbf{sistematica}:
\begin{itemize}
    \item se la risposta è \textit{sì} l'algoritmo deve terminare;
    \item se la risposta è \textit{no}, va bene se non termina ma tutti gli stati raggiungibili devono essere visitati nel limite: man mano che il tempo va all'infinito, tutti gli stati vengono visitati.
\end{itemize}

\subsubsection{Complessità spaziale e temporale}
\textcolor{blue}{Complessità spaziale}: come cresce la quantità di memoria richiesta dall'algoritmo di ricerca in funzione della dimensione del problema (caso peggiore)?\\ \textcolor{red}{Complessità temporale}: come cresce il tempo richiesto (numero di operazioni) dell'algoritmo di ricerca in funzione della dimensione del problema (caso peggiore)?
\\ \textbf{Trend asintotico}:
\begin{itemize}
    \item La complessità viene descritta con una funzione $f(n)$.
    \item Ai fini dell'analisi risulta conveniente adottare quella che si chiama la notazione "O-grande".
    \item $n$ di solito codifica la dimensione di una istanza del problema.
\end{itemize}
\section{Search: UCS e A*}
\subsection{Uniform Cost Seatch (UCS)}
Nell'albero di ricerca, teniamo traccia del nostro accumlato sul percorso dal nodo iniziale a ogni noto $V$: $g(V)$. Non consideriamo EQL. \\
L'UCS consiste nella selezione (espansione) del nodo con $g$ minore ancora da espolare (sulla frontiera). Goal check: se il \textbf{nodo selezionato per l'espandione} è un goal, mi fermo e restituisco la soluzione. \\
Ci si pone la domanda: abbiamo trovato il percorso ottimale per l'obiettivo? Per dare una risposta, possiamo ispezionare il grafico.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Images/UCS.png}
\end{figure}
Come si nota dal grafico, sì, trova la soluzione ottimale.
\\
Anzi possiamo affermare che: \textcolor{red}{ogni volta che UCS \textbf{seleziona} per la prima volta un nodo per l'espansione, il percorso che, sull'albero di ricerca, porta a quel nodo ha un \textbf{costo minimo}}. 
\subsubsection{Ottimalità di UCS}
\begin{figure}[H]
    \includegraphics[width=0.5\linewidth]{Images/Ottimilita_UCS.png}
\end{figure}
\textbf{Overload della notazione}: estendo la notazione di $g$ rendendola applicabile anche ai path $g(X → Y → \dots → Z) = g(Z))$.
\\ \textbf{Ipotesi:}
\begin{enumerate}
    \item UCS seleziona per la prima volta dalla frontiera un nodo $V$ che è stato generato attraverso un percorso $p$;
    \item il percorso $p$ non è il percorso ottimo per raggiungere $V$: $p^* \neq p$;
\end{enumerate}
Dato il secondo punto e la \textcolor{green}{\textbf{separation property}} della frontiera, sappiamo che deve esistere un nodo $X$ sulla frontiera, generato attraverso un cammino $p_1^*+ p_2^*$. \\
$p*$ è il \textbf{path ottimo}, quindi $g(p_1^*)<g(p_1*)+\Delta{p_2^*} < g(p)$.
\\ I costi sono tutti positivi, quindi $g(p_1^*)<g(p_1*)+\Delta{p_2^*} < g(p) \Rightarrow g(p_1*) <g(p)$.
\\ Questo implica che $g(X)<g(V)$, \textcolor{red}{la prima ipotesi è violata}.
\\ Se quando selezioniamo per la prima volta un nodo scopriamo il percorso ottimo, non c'è motivo di selezionare lo stesso nodo una seconda volta, introduciamo quindi la lista dei \textbf{nodi espansi}: \textbf{EXL}.
\\ Ogni volta che selezioniamo un odo per l'estensione:
\begin{itemize}
    \item Se il nodo è già in EXL, lo \textbf{scartiamo}.
    \item Altrimenti lo estendiamo e lo insieriamo in EXL.
\end{itemize}

\begin{figure}[H]
    \includegraphics[width=1\linewidth]{Images/UCS_conEXL.png}
\end{figure}

\subsubsection{Implementazione}
\begin{figure}[H]
    \includegraphics[width=1\linewidth]{Images/implementazione_UCS.png}
\end{figure}
\subsection{Ricerca informata}
\subsubsection{Ricerca non informata e non informata}
Gli algoritmi di ricerca decidono quale nodo espandere attraverso delle regole che applicano in funzione della conoscenza del problema e del processo di ricerca svolto fino al tempo presente.
\\ Una ricerca è \textcolor{blue}{\textbf{non informata}} se utilizza solo la conoscenza del problema che è specificata nella sua definizione.
\\ Una ricerca è \textcolor{red}{\textbf{informata}} va oltre alla definizione del problema sfruttando della conoscenza aggiuntiva: ciò che quel grafo, quelle connessioni e quei costi rappresentano nel mondo reale, oltre il formalismo agnostico che li esprime. 
\\ Dato un generico stato $S$, usando questa conoscenza, un algoritmo informato \textbf{stima} la bontà di $S$ attraverso una funzione $f(S)$ e guida la ricerca usando $f$.
\\Approccio \textbf{best-first}: espandere prima gli stati che hanno una $f$ migliore.
\\ Esistono diversi algorimti di ricerca best-first, la differenza la fa il \textbf{come $f$ è definita}.
\subsection{A*}
La forma più diffusa di ricerca best-first è la \textbf{ricerca A*}. La valutazione dei nodi viene eseguita combinando $g(n)$, il costo per raggiungere il nodo, e $h(n)$ (\textbf{euristica}), il costo per andare da lì all'obiettivo:
\[f(n)=g(n)+h(n)\]
Dal momento che $g(n)$ fornisce il costo di cammino dal nodo iniziale al nodo $n$, e $h(n)$ rappresenta il costo stimato del cammino più conveniente da $n$ all'obiettivo, risulta $f(n) = $ costo stimato della soluzione più conveniente che passa per $n$.
Se stiamo cercando di trovare la soluzione meno costosa, quindi una cosa ragionevole è provare per primo il nodo col valore più basso di $g(n)$ e $h(n)$. In effetti rilta che questa strategia è molto più che ragionevole, a patto che la funzione auristica $h(n)$
soddisfi certe condizioni, la ricerca A* è sia completa che ottima.
\subsubsection{Ammisibilità di A*}
A* è ottima se $h(n)$ è una \textbf{euristica ammissibile}, ovvero se $h(n)$ \textit{non sopravvaluta} mai il costo reale di una soluzione che passa per il nodo $n$.

\subsubsection{A* con EXL}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\linewidth]{Images/A_conEXL.png}
\end{figure}
Grazie all'ammisibilità manteniamo la stessa proprietà di ottimalità che abbiamo dimostrato con UCS: se non sovrastimiamo non possiamo scartare il path ottimo. \\
Se lavoriamo con EXL l'ammissibilità \textbf{non garantisce} l'ottimalità. Per risolvere ciò bisogna aggiungere all'euristica una proprietà più stringente: la \textcolor{red}{\textbf{consistenza}}.
\\ Siano $V$ e $U$ due stati connessi da una azione $a$. Una euristica $h$ è \textbf{consistente} se per ogni possibile coppia di $V$ e $U$ vale la seguente diseguaglianza: $h(V) \leq c(V, a, U)+h(U)$. (diseguaglianza triangolare)
\\
\begin{frame}{Disuguaglianza triangolare}
\end{frame}
Afferma che ogni lato del triangolo non può mai essere più lungo della somma degli altri due. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.25\linewidth]{Images/consistenza.png}
\end{figure}
\subsubsection{Ottimalità di A*}
Se assumiamo che $h$ sia consistente, possiamo costruire una dimostrazione per l'ottimalità di A*. \\ Cominciamo con il derivare una proprietà di $f$:
\begin{enumerate}
    \item Consideriamo due stati $V$ e $U$ connessi da un'azione $a$ che l'algoritmo ha generato uno dopo l'altro sull'albero di ricerca.
    \item Per definizione $f(U)=g(U)+h(U)$.
    \item Per definizione di $g$ e nostra azzunzione in 1, $g(U)=g(V) + c(V,a,U)$.
    \item Sostituiendo in 2: f(U)=g(V) + \textcolor{blue}{c(V,a,U)+h(U)}.
    \item Per la proprietà di \textcolor{blue}{\textbf{consistenza}} $c(V,a,U)+h(U) \geq h(V)$.
    \item \textcolor{green}{Sommando} $g(V)$ a entrambi i termini: \textcolor{green}{$g(V)+$} $c(V,a,U)+h(U) \geq \textcolor{green}{g(V)}+ h(V)$.
    \item \textcolor{red}{\textbf{$f(U) \geq f(V) \Rightarrow$}} \textbf{lungo ogni perorso nell'albero di ricerca la $f$ è monotono non decrescente}.
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.25\linewidth]{Images/assimilitaA.png}
    \end{figure}
\end{enumerate}

Estendo la notazione di $f$ esplicitando il path su cui si calcola il costo $g$: $f(p,n)=g(p)+h(n)$
\\ \textbf{Ipotesi:}
\begin{enumerate}
    \item A* seleziona per la prima vola dalla frontiera un nodo $V$ che è stato generato attraverso un percorso $p$.
    \item il percorso $p$ non è il percorso ottimo per raggiungere $V$: $p^*\neq p$
\end{enumerate}
Dato 2 e la \textcolor{green}{\textbf{separation property}} della frontiera, sappiamo che deve esistere un nodo $X$ sulla frontiera che si trova sul cammino ottimo $p^*=p^*_1+p^*_2$ verso $V$;
\begin{figure}[H]
    \centering
    \includegraphics[width=0.50\linewidth]{Images/OttimalitaA2.png}
\end{figure}

\begin{enumerate}
    \item $g(p)>g(p^*)$ perch+ sia $p^*$ che $p$ sono path che portano a $V$, ma il primo è ottimo mentre il secondo no;
    \item $\textcolor{purple}{f(p^*, V)} \geq \textcolor{Turquoise}{f(p^*_1,X)}$ dalla consistenza di $h$ ($f$ monotona non decrescente);
    \item $\textcolor{red}{g(p)+h(V)} > \textcolor{purple}{g(p^*,V) + h(V)}$ sommando lo stesso termine ai membri di 1;
    \item $\textcolor{red}{f(p,V)}> \textcolor{purple}{f(p^x,V)} \geq \textcolor{Turquoise}{f(p^x_1,X)}$ mettedno insieme 3, 2 e la definizione di $f$;
    \item $f(p,V) > f(p^*_1,X)$ quindi $V$ non può essere stato scelto prima di $X$, \textcolor{red}{l'ipotesi 1 è violata}
\end{enumerate}
\subsection{Progettare un'eutistica}
Per capire come trovare un metodo per costruire buone euristiche, cerchiamo prima di capire come si può valutare una euristia: ci sono euristiche che sono migliori di altre? E come si stabilisce?
\\ Possiamo immaginare la nostra $h$ come un punto in un intervallo limitato:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/creareEuristica.png}
\end{figure}
Vorrei spingere il più possibile a destra il punto, mantendendo però l'\textbf{efficienza} nel computare $h$. Perchè?
\\ A* espande tutti gli stati $S$ tali per cui $f(S)<g^*(goal)$ cioè tutti gli stati $S$ tali per cui $h(S)<g^*(goal)-g(S)$. QUinsi se per ogni stato $S,h_1(S) \leq h_2(S)$ allora $h_2$ domina $h_1$ e A* con $h_2$ non espanderà ppiù nodi di A* con $h_1$ (sempre assumendo che entrambe le euristiche siano consistenti e calcolabili in tempo efficiente). \'E\ facile vedere che date due euristiche consistenti $h_1$ e $h_2$ dove nessuna domina l'altra, possiamo costruire una terza euristica che le domina entrambe: $h_3 = max\{h_1,h_2\}$
\\ Il principio "euristiche più grandi sono migliori" ci permette di valutare le euristiche o di combinarle, ma come si costruisce una euristica da zero?
\\ Tale compito sembra essere piuttosto complesso: l'euristica srutta la struttura di un problema e deve soddisfare una serie di vincoli. 
\subsection{Rilassamento di un problema}
Dato un problema $P$, un \textbf{rilassamento di P} è una versione più semplice di $P$ in cui alcuni vincoli sono stati rimossi.
\begin{figure} [H]
  \centering\includegraphics[width=0.75\linewidth]{Images/rilassamento.png}
\end{figure}
\textbf{Idea}:
\begin{enumerate}
    \item Costruire un rilassamento di $P$: $\hat{p}$.
    \item Eseguire A* (o un qualsiasi altro algoritmo di ricerca) sul problema rilassato e trovare il costo ottimo da ogni nodo $S$ per arrivare al goal: $\hat{h}^*(S)$.
    \item Assegnare $h(S) = \hat{h}^*(S)$ ed eseguire A* con tale euristica.
\end{enumerate}
Possiamo facilmente definire un problema di rilassamento, si tratta solo di rimuovere vincoli/riscrivere i costi. Ma cosa succede alle proprietà delle'euristica e all'ottimalità di A*?
\begin{enumerate}
    \item Nel problema rilassato risolvo per il costo ottimo da ogni stato $\hat{h}^*(S) \leq \hat{c}(S,a,U) + \hat{h}^*(U)$.
    \item Dalla nostra idea $h(S)\leq \hat{c}(S,a,U)+h(U)$.
    \item Dalla definizione di rilassamento $\hat{c}(S,a,U) \leq c(S,a,U)$.
    \item \textbf{L'euristica h è consistente} $h(S)\leq c(S,a,U)+h(U)$.
\end{enumerate}
\break
\section{Bounded Suboptimal Search e varianti di A* }
\subsection{Limiti di A*}
L'uso di euristiche ammissibili costituisce un vantaggio perchè garantisce l'ottimalità. Può essere anche essere un grosso limite per mancanza di flessibilità. 
\\ Consideriamo, ad esempio, una situazione in cui in frontiera non ci sono un gran numero di nodi:
\begin{itemize}
    \item ciascun nodo rappresenta un path verso il goal:
    \item ciascun path ha più o meno lo stesso costo, uno solo ha il costo minimo.
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Images/limitiA.png}
\end{figure}
\textcolor{red}{A* spenderà un sacco di tempo a distinguere il path ottimo tra tutti questi path che sostanzialmente sono equivalenti}. NOn si accontenta di un path sub-ottimale, anche quando il suo
costo dista di pochissimo dall'ottimo.
\\ \textcolor{blue}{Come possiamo equipaggiare A* con un po' di flessibilità?}.
\subsection{Focal Search}
Supponiamo di avere a disposizione una seconda euristica, \textbf{non ammissibile}, che chiamiamo $\hat{h}_F$.
\\ $\hat{h}_F(n)$ è una stima del \textbf{costo computazionale} necessario per completare la ricerca del percorso ottimo verso il goal.
\\ Ricordando che, in generale, in un problema di search le azioni \textbf{non} hanno costi uniformi, possiamo dire che dato un nodo $n$:
\begin{itemize}
    \item $h(n)$ stima ottimisticamente il costo rimanente da spendere per arrivare al goal;
    \item $\hat{h}_F(n)$ stima il numero di azioni ancora da compiere per arrivare al goal, ovvero la lunghezza della parte di soluzione ancora da costruire.
\end{itemize} 
Più è lunga la parte di soluzione da costruire, più lavoro dovrò fare per costruirla! Questa valutazione non è legata al costo rimanente.
\\ \textcolor{green}{\textbf{Esempio: Traveling Salesman Problem}}
\begin{figure}[H]
    \includegraphics[width=1\linewidth]{Images/salesman.png}
\end{figure}
\begin{itemize}
    \item $F$: la lista di nodi in frontiera, quella usata da A*.
    \item $n_{best} = arg min_{n \in F} f(n)$
    \item $\omega \geq 1$, un parametro che scegliamo noi 
    \item $FOCAL \subseteq F$ sotto-lista definita cosi: \[FOCAL = \{n \in F | f(n) \leq \omega f(n_{best})\} \]
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Images/focal.png}
\end{figure}
\textbf{Regola di espansione}: scegliere la $FOCAL$ il nodo che minimizza $\hat{h}_F, n_{next} = arg min_{n \in FOCAL} \hat{h}_F(n)$.
\\ Tutto il resto resta uguale ad A*.
\\ \textcolor{red}{\textbf{Perdiamo l'ottimalità!}} Quando l'algoritmo seleziona per l'espansione un nodo di goal $e$ potrebbe non aver trovato il percorso ottimo
(potremmo non aver scelto il nodo con $f$ minima in frontiera e quindi potremmo aver saltato una linea di costo!)
\\ \textcolor{ForestGreen}{\textbf{La persita di ottimalità non è arbitrariamente grande}}, ma controllabile con il parametro $\omega$.
\subsubsection{Focal Search è una BSS}
\begin{itemize}
\item $e$: il nodo di goal selezionato da FOCAL (fa terminare la ricerca)
\item $n*$: il nodo che conduceva al goal su path ottimo, è in frontiera ma (assumiamo) non sia stato scelto
\item $OPT$ è il costo della soluzione ottima
\item $f(n^*)\leq OPT$ perchè $f$ usa un'euristica ammissibile
\item $f(n_{best}) \leq f(n^*)$ per definizione di $n_{best}$
\item $f(e)\leq \omega f(n_{best})$ per costruzione dell'algoritmo Focal Search
\item Mettendo insieme le disuguaglianze di cui sopra otteniamo: \[ g(e)=f(e)\leq \omega f(n_{best})\leq\omega f(n^*) \leq \omega OPT \Rightarrow g(e) \leq \omega OPT \]
\end{itemize}
Il costo della soluzione trovata da Focal Search può essere al più $\omega$ volte peggiore dell'ottimo
\\ BSS: Bounded Subotimal Search
\subsection{Problema del trashing}
La funzione $f$ può solo crescere. Questo implica, in $FOCAL$ search due effetti:
\begin{itemize}
    \item in $FRONTIER$ il nodo $n_{best}$ tenderà a stare a profondità bassa e quindi ad avere un valore alto di $\hat{h}_F$ (resta in FOCAL a lungo senza esser scelto)
    \item i nodi generati da una espansione tenderanno a non entrare in $FOCAL$ per molto tempo (fino a quando $n_{best}$ non viene rimosso).
\end{itemize}
Effetto "fisarmonica": $FOCAL$ viene svuotata, nel momento in cui viene rimosso $N_{best}$ viene riempita di nuovo, poi ancora svuotata, ... .
\\ Causa del problema: Focal Search usa $f$ per riempire $FOCAL$, questo sembrerebbe l'unico modo per garantire il bound della subottimalità, \textbf{ma non lo è}.
\\ Idea: usare $f$ solo per garantire il bound e usare, in ordine, $\hat{h}_F$, e una euristica "aggressiva" $\hat{h}$ per scegliere il nodo da espandere, EES (Explicit EStimation Search, 2011)
\subsection{Altri algoritmi di ricerca}
I vari concetti che definiscono gli algoritmi di ricerca visti in precedenza, possono essere visti come il loro building blocks.
\\ NUlla vieta di combinarli in modi alternativi per definire nuovi algoritmi di ricerca.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/moreAlgoritmi.png}
\end{figure}
\section{Giochi: nozioni di base, Minimax e alfa-beta pruning} 
\subsection{Adversarial Search}
L'adversarial search è una \textbf{ricerca con avversari} creata in un \textbf{ambiente competitivo} in cui vi sono due o più agenti con obiettivi in conglitto.
\\ Consiste in un ambiente multi-agente in cui gli obiettivi degli altri obiettivi non sono necessariamente concordanti con quelli del nostro agente di riferimento. In questi casi si parla di un sistema
multi-agente e l'interazione tra di esse può essere chiamata \textbf{gioco}.
\subsection{Giochi}
Un gioco è un problema di decisione \textbf{interattivo}. Ogni agente ha le proprie preferenze (utilità) individuali sugli stati del mondo. Le azioni di un agente influenzano l'ambiente e quindi, indirettamente, 
anche gli altri agenti. \\ Nel cercare una sequenza di azioni verso l ostato desiderato, l'agente da noi controlalto deve considerare le \textbf{strategie} degli altri decisori.
\\ Esistono diversi tipi di giochi:
\begin{itemize}
    \item 2 o \textit{n} giocatori;
    \item Agenti \textbf{razionali}, $\epsilon$-razionali;
    \item Struttura sequenziale: \textbf{turni}, azioni simultanee, ...;
    \item \textbf{Deterministico} o stocastico: tutte le azioni hanno effetti prevedibili?
    \item Stuttura dei payoff: \textbf{somma costante} o somma generica;
    \item \textbf{Informazione completa}/incompleta: i giocatori conoscono/non conoscono gli obiettivi e le azioni disponibili agli altri agenti;
    \item \textbf{Indormazione perfetta}/imperfetta: i giocatori sono/non sono informati di tutto quello che è succesos af ogni punto del gioco.
\end{itemize}
Lo studio del comportamente individuale dei singoli agenti si chiama \textcolor{red}{\textbf{teoria dei giochi competitivi}}.
\\Lo studio delle dinamiche nella formazione di coalizioni si chiama \textcolor{Aquamarine}{\textbf{teoria dei giochi cooperativa}}.
\\I giochi più studiati in IA sono quelli che gli studiosi di teoria dei giochi chiamano deterministici, a due giocatori, a turni, con \textbf{informazione perfetta} (completamente osservabile), \textbf{a somma zero} (ciò che a vantaggio di un giocatore danneggia l'altro).
\newpage
Consideriamo giochi formalizzati così:
\begin{itemize}
    \item \textbf{Stati}: $S=\{s_1,s_2,\dots\}$ insieme o \textbf{spazio} degli stati, dove $s_k \in S$ è lo stato \textbf{iniziali} (la situazione di partenza in cui trovano gli agenti e ambiente).
    \item \textbf{Giocatori e azioni possibili}: insieme di agenti $I=\{i_1,i_2\}$, insieme di azioni disponibili $A=\{a_1,a_2,a_3,\dots\}$.
    \item \textbf{Turni e azioni legali}: dato $s_k \in S$, $I(S_k)\in I$ è il giocatore che hanno diritto di compiere un'azione (turno) e $A(s_k)$ è l'insieme di azioni che può intraprendere.
    \item \textbf{Modello di transizione}: dato $s_k \in S$ e $a \in A(I(s_k))$, $f(s_k,a)\in S$ indica il prossimo stato del gioco e cioè il risultato della mossa $a$.
    \item \textbf{Stati terminiali}: dato uno stato $s_k$, $T(s_k)=1$ se $s_k$ è uno stato terminale, dove il gioco termina e un payoff viene inviato ad ogni giocatore; vale 0 altrimenti.
    \item \textbf{utilità}: dato uno stato terminare $s_t$, $u_i(s_t)$ è il payoff che il giocatore $i$ riceve in quello stato.
\end{itemize} 
Tutti questi punti prendono il nome di \textbf{meccanismo}.
\\ Questa formalizzazione implica: 2 giocatori, struttura sequenziale a turni alternati e azioni deterministiche.
\\ I giochi hanno anche altre caratteristiche:
\begin{itemize}
    \item \textbf{Informazione completa}: entrambi i giocatori hanno acceso al meccanismo, conoscono azioni possibili e utilità del proprio avversario.
    \item \textbf{Informazione perfetta}: dato uno stato corrente del gioco $s_j$ ogni giocatore ha accesso allo stato e alla sequenza di azioni che, a partire dallo stato iniziale, ha portato dino ad $s_k$.
    \item \textbf{A "somma zero"}: significa che in ogni stato terminale $s_t$ vale $u_1(s_t)+U_2(s_t)=0$.
    \begin{itemize}
        \item schema di competizione pura: il guadagno di un agente corrisponde ad una egual perdita dell'avversario;
        \item da un punto di vista matematico \textbf{è equivalente ad un gioco a somma costante} $u_1(s_t)+u_2(s_t)=C$
        \item Interpretazione: ogni giocatore paga una quota di $\frac{C}{2}$ per entrare nel gioco, poi $C$ viene ridistribuito a seconda dell'esito;
        \item Il gioco a somma costante $C$ può essere sempre trasformto in un gioco a somma zero rinormalizzando i payoff con una trasformazione affine;
        \item In un gioco a somma zero l'utilità del secondo giocatore è implicita, si indica solo con $u_1$, come se ci fosse un trasferimetno di utilità da $i_1$ a $i_2$. \textcolor{ForestGreen}{Esempio: $u_1=5 \rightarrow i_2$ paga $5$ a $i_1$, $u_1=-5 \rightarrow i_1$ paga $5$ a $i_2$}.
        \item Entrambi \textbf{massimizzano la loro utilità}, ma per $i_2$ (quello di cui non esplicitiamo l'utilità) vale: $max\{u_2\} = max\{-u_1\}= -min\{u_1\}$, quindi possiamo dire che cercherà di minimizzare $u_1$ così da ricevere il miglior posto possibile $-u_1$.
    \end{itemize}
\end{itemize} 
Il meccanismo descrive le "regole" del gioco, ma rappresenta solo una parte della sua descrizione.
L'altra parte è data dalle \textbf{strategie}.
\\ La strategia di un giocatore $i$ specifica il comportamente di quel giocatore in ogni possibile stato del gioco, dato $s_k$, tale per cui $I(s_k)=i,\sigma_i(s_k)$ è la strategia di $i$ nello stato $s_k$. Se la strategia coincide con una singola azione, quella da giocare in quel caso si chiama \textbf{strategia pura} $\sigma_i:\{s_k | I(s_k)=i\} \rightarrow A(s_k)$. Se invece la strategia è una distribuzione di probabilità su più azioni, si chiama \textbf{strategia mista} $\sigma_i:\{s_k|I(s_k)=i\} \rightarrow \Pi(A(s_k))$ (dove $\Pi(Q)$ è lo spazio di distribuzioni di probabilità sugli elementi di $Q$).
\\Ci concentreremo prinicipalmente sulle \textbf{strategie pure}
\\ \textbf{\textcolor{red}{Strategia ottima}}: quella strategia tale per cui ogni strategia diversa non introduce miglioramenti contro un avversario \textbf{infallibile}. 
\\Per ricolvere un gioco bisogna calcolare la strategia ottima.

\subsubsection{Albero di gioco}
Il meccanismo dà origine all'\textbf{\textcolor{red}{albero di gioco}}. Esso è un \textbf{albero di ricerca} che si ottiene sviluppando tutte le possibili sequenze di mosse alternate, in cui ogni nodo è un \textbf{\textcolor{red}{nodo di decisione}}: uno dei due giocatori deve fare la sua mossa.
\\ Ogni branch rappresenta un gioco (partita). Dallo stato iniziale fino a quello terminale dove il gioco termina e i payoff sono distribuiti, ogni foglia è uno stato terminale su cui indichiamo il payoff del giocatore $i_1$ (quello del giocatore $i_1$ è uguale all'oppsoto).
\textcolor{ForestGreen}{Esempio: il gioco del tris, Giocatore $i_1$: X, giocaotre $i_2$:0. Il branching massimo è b=9 e profondità massima d=9. Gli stati sono: \begin{itemize}
    \item $\leq 3^9=19683$
    \item rimuovendo gli stati illegali ne restano 5478, al netto delle simmetriche sussistono di fatto 5478, al netto delle simmetrie sussistono di fatto 765 diverse situazioni strategiche in cui decide.
    \end{itemize}
NUmero di nodi dell'albero di gioco: $\leq 1+9+(9*8)+(9*8*7)+(9*8*7*6)+ \dots = 986410$ se non facciamo ottimizzazioni è una stima ragionevole.
\\ Numero di giochi (ovvero i branch dell'albero o i suoi nodi terminali):\begin{itemize}
    \item $9*8*7*6*5*4*3*2*=9!=362880$
    \item numero esatto $255168$, al netto delle simmetrie $26830$.
\end{itemize}}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Images/alberoGioco.png}
\end{figure}
\subsection{Minimax}
Nei problemi di search l'albero di ricerca rappresentava/supportava il processo di inferenza per trovare una strada o la strada ottima per il goal. L'albero di gioco ha una funzione analoga: rappresenta/supporta il ragionamento strategico.
\\ Per adesso assumiamo che trovaer la strategia ottima sia un problema di search che tiene conto dell'avverario: \textbf{aversarial search}. L'approccio di base con cui si risolve la classe di giochi che stiamo considerando è dato dall'algormitmo \textbf{\textcolor{red}{minimax}}, sostanzialmente una DFS sull'albero di gioco con un \textbf{ritorno all'indietro} dei valori di utilità.
\\ È un approccio esautistivo e inefficiente che possiamo adottare solo per giochi di dimentisioni limitate, ma è un approccio esatto, corretto e completa; sta alla base dei metodi più sofisticati.
\\Introduciamo della notazione per semplificare la descrizione dell'algoritmo:
\\$I=\{\textcolor{red}{MAX},\textcolor{blue}{MIN}\}$, il giocatore $i_1$ si chiama \textcolor{red}{MAX} e il suo obiettivo nel gioco è ottenere un valore di utilità il più alto possibile. Il giocatore $i_2$ si chiama \textcolor{blue}{MIN} e il suo obiettivo è far sì che l'utilità di \textcolor{red}{MAX} sia la più bassa possibile
(implicitamente massimizza la sua utilità che, sotto l'assunzione di somma zero ,equivale a minimizzare l'utilità dell'avversario).
\\ Per semplicità \textcolor{red}{MAX} avrà sempre utilità $\geq 0$ (e di conseguenza quelle di \textcolor{blue}{MIN} saranno sempre $<0$).
Consideriamo giochi piccoli per ragioni di spazio, ma le considerazioni si generalizzano su ogni gioco.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/minmax.png}
\end{figure}
Procedimento:
\begin{enumerate}
    \item Eseguire una DFS sull'albero di gioco
    \item Riposrto all'indietro dei valori
\end{enumerate}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/analisiMinMax.png}
\end{figure}
\textbf{Lato negativo}: bisogna attraversare tutto l'albero. Il risolutore, a differenza della mente umana ha un limite molto maggiore di ragionamento per sviluppare la propria mossa. Spesso la decisione è estremamente controintuitiva. Questo approccio va bene finchè il numero di decisioni non aumenta.
\subsubsection{Minimax con avversario non razionale}
Minimax ragiona per trovare la strategia ottima assumendo che l'avversario giochi a sua volta la sua strategia ottima, date le nostre assunzioni sul meccanismo equivale a dure che l'avversario è \textbf{razionale}.
\\ Cosa succede se MAX ragiona assumendo un avversario razionale, ma poi, nel momento del gioco, il suo avversario non lo è? In tal caso la strategia Minimax non peggiora, ma \textcolor{red}{non è garantita che sia quella ottima}.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/MinMaxNonOttimo.png}
\end{figure}
Minmax è una strategia \textbf{conservativa}. Il valore Minimax è un Lower Bound sul valore del gioco.
\subsubsection{Minimax con più agenti}
Minimax può essere facilmente esteso ad una classe di giochi equivalente a quella che abbiamo considerato, ma dove passiamo da 2 a $n$ agenti giocatori. Non possiamo pià rappresentare le utilità rispetto ad un solo giocatore perchè il concetto di somma zero diventa più complicato.
\\ In ogni stato terminale ripostiamo un vettore di $n$ utilità, una per ciascun giocatore. 
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/minmaxmoreagents.png}
\end{figure}
L'algoritmo sceglierà quale valore propagare in base al payoff che otterebbe il giocatore che ha eseguito la mossa.
\subsection{$\alpha-\beta$ pruning}
\subsubsection{Migliorare le performance di Minimax}
Minimax ha il problema di eseguire uan DFS esaustiva su tutto l'albero di gioco per poter determinare la sua mossa. La complessità è quindi esponenziale nella profondità dell'albero di gioco: $O(b^d)$.
\\ Per certi giochi quindi la complessità combinatoria elevata rappresenta un limite \textbf{impossibile} da valicare con l'approccio Minimax.
\\ \textbf{Idea: } nei problemi di search abbiamo definito delle regole di \textbf{prnunig} che ci hanno aiutato a migliorare l'efficienza pratica degli algoritmi: possiamo definire regole analoghe per la risoluzione di giochi?
\\ Consideriamo questa situazione su un albeero di gioco esplorato da Minimax
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/migliorieMinimax.png}
\end{figure}
Questo schema può essere formalizzato e codificato in una regola di pruning.
\subsubsection{Potatura $\alpha-\beta$}
\textbf{Principio dell'$\alpha-\beta$ pruning}: man mano che la DFS procede, teniamo traccia, su ogni nodo del branch corrente, di ciò che sappiamo sul valore di quel nodo. Fino a che la DFS non ha esplorato tutti i nodi figli, non conosciamo con esattezza il valore del nodo. Ma qualcosa sappiamo! 
\\ Anzichè un valore singolo, usiamo un intervallo di valori $[\alpha, \beta]$: indica che, stando a quanto scoperto dalla DFS fino a quel momento, il valore Minimax di quel nodo è compreso nell'intervallo (inizialmente è $[- \infty, + \infty]$).
\\ In molti casi, per scartare un nodo dall'albero di gioco non ci interessa conoscere l'esatto valore Minimax, ci basta $[\alpha, \beta]$.
\begin{itemize}
    \item $\alpha$: il minimo valore garantito per MAX.
    \item $\beta$: il massimo valore garantito per MIN.
\end{itemize}
\newpage
\subsubsection{Esempio}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/alphabeta.png}
\end{figure}
La potatura $\alpha-\beta$ può introdurre diversi vantaggi, ma ha un problema di fondo sul quale ci si può fare poco: \textbf{almeno una porzione dell'albero di
gioco va esplorata fino ai nodi terminali}. Questa soluzione risulta impossibile in molti casi reali per due ragioni:
\begin{enumerate}
    \item Complessità elevatissima dell'albero di gioco;
    \item Tempo limitato entro cui va presa la decisione su quale mossa fare.
\end{enumerate}
C'è un aspetto più di fondo: risolvere il gioco non è poi così interessante. Immaginiamo due giocatori ideali e cioè entrambi in grado di risolvere il gioco all'esatto, la partita può andare
solo in tre modi: uno dei due si arrende o si mettono d'accordo per pareggiare.
\\ Ricapitolando:
\begin{enumerate}
    \item Sarebbe bello avere un \textbf{ordering perfetto} delle mosse, ma per averlo equivale a risolvere all'esatto;
    \item Sarebbe bello poter risolvere \textbf{all'esatto}, ma non abbiamo abbastanza risorse computazionali.
\end{enumerate}
\textbf{Domanda sul problema 1}: sarebbe possibile calcolare un ordine delle azioni man mano che la ricerca procede? Non l'ordine perfetto, ma uno euristico, informato da ciò 
che la ricerca ha scoperto fino a quel momento.
\\ \textbf{Domanda sul problema 2}: sarebbe possibile assegnare un valore numerico anche ad un nodo non terminale $p$ dell'albero di gioco, in modo da stimare il valore Minimax senza dover 
scendere in profondità fino ad almeno una foglia del sotto-albero con radice in $p$?
\\ La risposta ad entrambe le domande è sì e la tecnica per farlo rende la potatura $\alpha,\beta$ più efficiente combinando tre elementi:
\begin{itemize}
    \item funzioni di \textbf{valutazione} e \textbf{cutoff};
    \item tabelle delle \textbf{trasposizioni};
    \item \textbf{iterative deepening}.
\end{itemize}
\subsubsection{Funzioni di valutazione e cutoff}
Dato un nodo $s$ dell'albero di gioco, anzichè calcolare il suo valore Minimax fornisco una stima di quel valore applicando una funzione di valutazione \textcolor{red}{$v(s)$}. La funzione $v$ è un'\textbf{euristica}: produce 
una stima della qualità di quel nodo, codifica, in una computazione \textbf{efficiente}, quello che un giocatore umano fa quando cerca di intuire la bontà del trovarsi 
in una particolare situazione del gioco.
\\ Approccio: data $v$ e un \textbf{test di cutoff} $CUT(s,d)$ per un nodo $s$ alla profondità $d$, se il test restituisce false, eseguo Minimax da quel nodo, altrimenti fermo Minimax e restituisco $v(s)$.
\\ Introduciamo incertezza: la complessità computazionale mi impedisce di calcolare il valore esatto di ogni nodo, anche se, viste le assunzioni sul meccanismo, sarebbe possibile. Se la funzione di valutazione è \textbf{efficace},
posso riuscire comunque a identificare buone mosse.
\begin{center}
    \textit{Posso fermare la ricerca dopo un tempo ragionevole e fornire comunque un'azione da giocare!
    \\ Posso usare $v$ per ordinare le azioni e valutare prima quelle che sembrano migliori!}   
\end{center}
Dato uno stato si possono definire delle \textbf{feature} che lo descrivono. \textcolor{ForestGreen}{Ad esempio scacchi: $(n_1,n_2,\dots, n_6)$, dove $n_i$ è il numero di pezzi del tipo $i$ rimasti nella scacchiera.}
\begin{itemize}
    \item funzione basata sull' \textcolor{blue}{\textbf{esperienza}} (tante partite che abbiamo giocato e salvato in un dataset):
    \begin{itemize}
        \item Ogni profilo di feature $p$ definisce una classe di equivalenza tra stati;
        \item Dall'esperienza sappiamo che di tutte le occasioni in cui ci siamo trovati in uno stato con profilo $p$, nel $50\%$ dei casi abbiamo vinto, nel $20\%$ abbiamo perso, mentre nel restante $30\%$ abbiamo pareggiato;
        \item Quindi se $s$ è un nodo con profilo $p$, possiamo assegnargli il valore $v(s)= 0,5*1+0,2*(-1)+0,3*0=0,3$.
\end{itemize}
\item funzione basata sulla \textcolor{red}{\textbf{combinazione}} di feature:
\begin{itemize}
    \item Se chiamo $f_1(s),f_2(s),\dots,f_n(s)$ le $n$ feature che ho definito per un generico stato $s$, posso aggregarle per ottenere un valore: 
    \[ v(s)=\omega_1f_1(s)+\omega_2f_2(s)+\dots + \omega_nf_n(s)\]
\end{itemize}
\end{itemize}
Nella pratica queste funzioni sono convenienti se:
\begin{itemize}
    \item Riusciamo a definire buone feature \textbf{manualmente}.
    \item Si può calcolare in modo \textbf{efficiente} le funzioni di valutazione (tempo polinomiale con esponente basso).
\end{itemize}
Non sempre questi due scenari si verificano, in certi casi non è ovvio come buone feature siano definite, serve un approccio alternativo per identificarle (es. deep learning).
\\ Come definire il test di cutoff $CUT(s,d)$? Basato su soglia di profondità \textbf{massima}.
\\ Identificazione degli \textbf{stati quiescenti}: gli stati in cui non ci si aspettano grandi cambiamenti in termini di valore.
\\ \textcolor{red}{\textbf{Quiescent search}}: esegue un cutoff restituendo $v()$ solo per gli stati "quieti", dove non ci sono minacce o opportunità imminenti. 
Se uno stato non è quiescente, si va avanti in profondità.\\ L'arroccio della quiescent search sfrutta una caratteristica ricorrente in molti giochi: gli episodi rilevanti si sviluppano in regioni di profondità relativamente 
compatte e distanziate:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/quiescent.png}
\end{figure}
\pagebreak
\subsubsection{Trasposizoni}
Molti nodi nell'albero di ricerca sono strategicamente equivalenti perchè identici o per via di simmetrie e regolarità.
\\ \textcolor{ForestGreen}{Esempio: questi due nodi sono equivalenti a meno di una rotazione, dal punto di vista strategico sono lo stesso}
\begin{figure}[H]
    \centering
\includegraphics[width=0.30\linewidth]{Images/tris.png}
\end{figure}
In altri casi uno stesso stato può essere raggiunto da sequenze diverse di mosse, in generale uno stato che richiede $m$ mosse da parte di ogni giocatore può ammettere fino a $m!^n$ diverse sequenze
(dove $n$ è il numero di giocatori).
\\ \textbf{Trasposition table}: hash table dove per ogni stato $\overline{s}$ valutato dalla ricerca mantengo questi campi:
\begin{itemize}
    \item Il valore dello stato $\overline{v}$ e la mossa $\overline{a}$ associata a quel valore.
    \item La profondità massima $\overline{d}$ da cui è arrivata la ricerca che ha valutato quel nodo (dipende dal cutoff).
\end{itemize}
Se durante la ricerca (con limite di profondità $\overline{d}$) tutto il sotto-albero è stato esplorato fino ai nodi terminali, allora $\overline{v}$ è il valore
Minimax del nodo; in tutti gli altri casi è un bound ($\geq$ o $\leq$ a seconda se il nodo è MIN o MAX)
\\ È possibile $\overline{v}$ sia stato determinato dopo una potatura $\alpha$ (se il nodo è MIN) o $\beta$ (se il nodo è MAX), quindi non è detto che tutte le azioni siano state valutate.
\subsection{Giochi stocastici, incertezza}
Introduciamo l'\textbf{incertezza} nel nostro modello di gioco, questa nuova caratteristica ha un duplice ruolo:
\begin{itemize}
    \item ci permette di avere un modello più generale con cui descrivere i giochi dove alcune dinamiche sono random;
    \item ci permetterà di definire un nuovo approccio per la risoluzione dei giochi non più basato sulla ricerca esatta, ma su una ricerca "probabilistica" che può essere usato sia con giochi stocastici che non.
\end{itemize}
Come si introduce l'incerteza nei giochi? Si introduce un nuovo giocatore: \textbf{la Natura: $\mathcal{N}$}.
\\ $\mathcal{N}$ può essere pensata come un giocatore analogo a tutti gli altri: avrà i suoi turni di gioco e, di conseguenza, i suoi nodi di decisione sull'albero di ricerca (detti anche \textbf{nodi di chance}).
\\ Tuttavia, rispetto agli altri giocatori presenta \textcolor{red}{\textbf{tre}} differenze fondamentali:
\begin{itemize}
    \item La sua strategia di gioco è \textbf{fissa a priori} ed è conosciuta da tutti gli altri giocatori da prima che il gioco inizi (come se $\mathcal{N}$, prima di iniziare la partita, dichiarasse pubblicamente
    la sua strategia e, durante il gioco, rispettase la promessa).
    \item La sua strategia di gioco è \textbf{mista} (una distribuzione di probabilità sulle azioni), quindi nel momento di decidere l'azione si lanciano dei dadi.
    \item $\mathcal{N}$ non riceve payoff, è totalmente agnostica rispetto alle dinamiche del gioco , quindi la sua strategia è un \textbf{fenomeno dato} non il risultato di un ragionamento rispetto a delle preferenze.
\end{itemize}
\textcolor{Fuchsia}{È sostanzialmente l'ambiente, a cui non frega di vincere o di perdere}
\pagebreak
\\ Come cambia la rappresentazione grafica del gioco?
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/giocoConIncertezza.png}
\end{figure}
Le mosse colorate in nero sono quelle dell'ambiente, nodi di chance. Gli agenti si trovano in un ambiente \textbf{stocastico}

\subsubsection{Expectimax}
Visto che $\mathcal{N}$ non ha un comportamento strategico, la logica Minimax è ancora valida: possiamo estendere l'algoritmo in modo che gestisca l'incertezza.
\\ \textcolor{red}{\textbf{Idea}}: Minimax dove il valore dei nodi di chance non è determinato con il \textit{max} o \textit{min} dei valori sottostanti, ma con il \textbf{valore atteso} $E[X]$ definito come la somma
dei valori sottostanti, ciascuno moltiplicato per la probabilità di finire in quel nodo.
\\ Nel caso di somma-zero a due giocatori con turni alternati (che includano anche $\mathcal{N}$), l'algoritmo Minimax si estende naturalmente e diventa \textcolor{red}{Expectimax}.
\\ Problema: la potatura $\alpha,\beta$ sotto un nodo di chance (e quindi in generale) \textbf{non si può applicare} perchè la scelta dell'azione non è guidata da un giocatore. Il valore che "sale" dai nodi sottostanti cambia
se ne scartiamo alcuni. Questo non valeva con MAX o MIN perchè il massimo/minimo tra $n$ valori rimane lo stesso se, tra quegli $n$, si scartano i valori che non sono il massimo/minimo.
\\ Se ci sono date assunzioni sui payoff però si può fare!
\\ \textcolor{ForestGreen}{\textbf{Esempio} supponiamo di sapere che $10\leq u_1 \leq 30$ in qualsiasi terminale}.  
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/alfabeta_stocastico.png}
\end{figure}

\subsubsection{Stimare il valore di un'azione}
L'incertezza non è una caratteristica del problema che dobbiamo gestire, ma può essere "sfruttata" nella definizione di un metodo risolutivo alternativo.
\\ Con l'eccezione di $PROBCUT$, tutti i metodi che abbiamo studiato si basano su inferenze esatte; tutti questi approcci soffrono di un problema di scalabilità.
\\ \textcolor{red}{\textbf{Idea}}: trasformare il processo di inferenza in un \textbf{processo di stima}: non cerco l'azione migliore, ma stimo la bontà di ogni azione e scelgo quella con il valore stimato più alto.
\\ Se il processo di stima è ben definito, con il tempo, i valori stimati convergono al \textcolor{red}{valore esatto}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Images/montecarlo.png}
\end{figure}
\begin{itemize}
    \item Considero un'azione e lo stato $s$ in cui mi porterebbe.
    \item Simulo un numero molto alto di partite a partire da $s$: gioco contro me stesso scegliendo le azioni in modo casuale.
    \item La media dei punteggi finali è una stima del valore di $s$.
    \item Ripeto per ogni azione e scelgo quella con la stima più alta.
    \item Questa idea si chiama \textcolor{red}{Monte Carlo Tree Search} (MCTS).
\end{itemize}
\subsubsection{Monte Carlo Tree Search (MCTS)}
Caso semplificato: stimare la \textbf{winning rate} di un'azione $a_i$, cioè la probabilità di vincere la partita se mi trovassi nello stato, che indico con $s_i$, in cui tale azione mi porta.
\\ Posso scegliere tra $n$ azioni ($a_1, a_2,\dots, a_n$) e ho un tempo limitato che consente di svolgere un totale di $N$ simulazioni (supponiamo di non poter parallelizzare).
\\ Appoccio naive: eseguo $\frac{N}{n}$ simulazioni per ogni $s_i$ e per ciascuno ottengo la stima della winning rate $\omega_i=\frac{nW_i}{N}$ dove $W_i$ è il numero di simulazioni in cui ho vinto;
alla fine gioco $a^*=argmax_i\{\omega_i\}$.
\\ \textcolor{red}{\textbf{Problema}}: sto suddividendo lo sforzo in modo uniforme e non informato, potrei spendere troppo tempo su azioni che è chiaro che nono sono buone e, viceversa, troppo poco tempo su azioni che sembrerebbero
buone e che richiederebbero maggior precisione di stima (più simulazioni delle altre).
\\ \textcolor{Green}{\textbf{Soluzione}}: concentrare più sforzo (numero di simulazioni) verso le azioni che si configurano come migliori.
\\ \textcolor{red}{\textbf{ATTENZIONE!}} Devo stare attento a non escludere del tutto le azioni che non sembrano buone perchè, magari, con più simulazioni posso scoprire che in realtà sono migliori!
Questo è un problema fondamentale dell'informatica: \textcolor{red}{\textbf{EXPLORATION VS ESPLOITATION}} dilemma.
\subsubsection{Multi-Armed Bandit Problem (MAB)}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/mab.png}
\end{figure}
Come spendere al meglio le $N$ giocate? \textcolor{red}{\textbf{Risultato fondamentale}}: simula l'azione che massimizza l'\textbf{Upper Confidence Bound} $UCB_i=\omega_i+c \sqrt{\frac{logN}{N-i}}$ dove $N_i$ è il numero di simulazioni
svolte per $a_i$ e $c$ è un parametro che bilancia exploration ed exploitation.
\subsubsection{MCTS con UCB}
Vantaggi:
\begin{itemize}
    \item \textbf{Anytime}: fermando l'algoritmo ad un qualsiasi tempo $t$ otteniamo la decisione migliore possibile con le informazioni raccolte fino a quel momento.
    \item Non necessita euristiche, funzioni di valutazione, definizione di feature, ...
    \item \textbf{Informato}: la ricerca si concentra maggiormente sui branch più promettenti (senza tralasciare mai completamente tutti gli altri).
\end{itemize}

\section{Constraint Satisfaction Problems}
Gli algoritmi di search si basano su una assunzione di fondo: \textbf{lo stato è un'entità atomica}, la cui struttura interna non è accessibile. Fare search vuol dire ragionare sulla struttura dello spazio degli stati
(come questi sono collegati tra loro) senza considerare il come uno stato codifica ciò che rappresenta. 
\\ Esistono però molti problemi in cui la struttura dello stato è \textcolor{red}{accessibile}. Anzichè creare una sequenza di transizioni per attraversare lo spazio degli stati verso un goal, cerchiamo di costruire/\textbf{identificare}
lo stato di un goal (soluzione) manipolando direttamente la sua struttura interna (search vs identification).
\\ Un CSP è un \textbf{sotto-insieme} di problemi di search dove stato, transizione e goal check hanno una struttura specifica:
\begin{itemize}
    \item \textbf{Stato} $\rightarrow$ insieme di variabili, inizialmente senza un valore assegnato.
    \item \textbf{Transizione} $\rightarrow$ assegnare un valore ad una variabile.
    \item \textbf{Goal check} $\rightarrow$ soddisfare un insieme di vincoli sugli assegnamenti di alcune o tutte le variabili.
\end{itemize}
\textbf{Risolvere un CSP} consiste quindi nel trovare un valore da assegnare ad ogni variabile in modo che nessun vincolo sia violato. Conosciamo la struttura interna del problema attraverso un formalismo più specifico di quello dei problemi
di search (dove stato, transizione e goal potevano essere arbitrari). È comunque un formalismo abbastanza generale (variabili, vincoli) da permetterci di modellare molti problemi interessanti.
\\ \textcolor{red}{\textbf{Vantaggio chiave}}: avere accesso alla struttura del problema ci permette di definire algoritmi risolutivi più efficienti (trovando assegnamenti illegali si possono escludere grosse regioni dello spazio degli stati).
\\ \textbf{Search vs CSP}
\\ \textcolor{ForestGreen}{Esempio: risolvere un puzzle da 12 pezzi.}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/puzzle.png}
\end{figure}
\textbf{Formalizzazione}

\begin{itemize}
    \item Insieme di \textbf{variabili} $X=\{X_1,X_2,\dots,X_n\}$. 
    \item Insieme di \textbf{domini} $D=\{D_1,D_2,\dots,D_n\}$.
    \item Insieme di \textbf{vincoli} $C$.
\end{itemize}
Un vincolo si rapprenta come una tupla di sue elementi: $\langle variabili, relazione\rangle$:
\begin{itemize}
    \item \textbf{variabili}: elenco delle $k\leq n$ variabili coinvolte nel vincolo.
    \item \textbf{relazione}: elenco delle $k$-tuple di valori che le variabili possono assumere.
\end{itemize}
\pagebreak
È un formalismo teorico con cui si può esprimere qualsiasi vincolo, ma non è efficiente. Restringeremo quindi il tipo di vincoli possibili e adotteremo un linguaggio specifico per quei vincoli.
\subsection{Esempi}
\subsubsection{Le N regine}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Images/regine1.png}
    \includegraphics[width=1\textwidth]{Images/regine2.png}
\end{figure}

\subsubsection{Graph coloring}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Images/coloring1.png}
\end{figure}
\subsubsection{Altri esempi}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Images/sudoku.png}
\end{figure}
\subsection{Tipi specifici di vincoli}
Il tipo di vincolo, diverso a seconda del problema che abbiamo, dipende dal numero di variabili che coinvolge.
\begin{itemize}
    \item Vincoli \textbf{unari}: coinvolgono una sola variabile.
    \item Vincoli \textbf{binari}: coinvolgono copppie di variabili.
    \item Esistono vincoli più generali che coinvolgono un numero maggiore di variabili. Un vincolo che può coinvolgere fino a \textbf{tutte} le variabili presenti in un CSP 
    si chiama vincolo \textbf{globale}.
\end{itemize}
Un CSP con domini infiniti può essere sempre convertito in uno equivalente con solo vincoli \textcolor{red}{binari}, a patto di introdurre un numero sufficiente di variabili ausiliarie.
\subsubsection{Convertire un CSP generico in uno binario}
\textcolor{ForestGreen}{Esempio: come converto il vincolo $A+B=C$ in uno binario?}
\\Nuova variabile ausiliaria $V$ il cui dominio è $D_A \times D_B$ cioè i "valori" assegnabili a $V$ sono tuple di 2 valori, uno del dominio di $A$ e uno del dominio di $B$.
\\ \textbf{Nuovi vincoli}: sia $V$ una variabile a $n$ dimensioni (cioè i suoi valori sono tuple di $n$ valori) e $X$ una variabile standard a 1 dimensione:
\begin{itemize}
    \item \textcolor{red}{$i(X,V)$} impone che $X$ sia uguale all'$i$-esimo valore della tupla assegnata a $V$; ad esempio se $V=(9,-11,42),2(X,V)$ impone che $X$ debba valere $-11$.
    \item \textcolor{red}{$\Sigma(X,V)$} impone che $X$ sia uguale alla somma di tutti i valori della tupla assegnata a $V$; ad esempio se $V=(9,-11,42), \sigma(X,V)$ impone che $X$ debba valere $40$.
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/vincoli.png}
\end{figure}
\subsection{Metodi risolutivi}
Visto che i CSP sono un sottoinsieme di problemi di search, possiamo sicuramente applicare un algoritmo di ricerca come, ad esempio, la DFS dove un goal è uno stato in cui tutte le variabili sono assegnate e i vincoli sono rispettati. \\
Nel caso pessimo vengono provati tutti gli assegnamenti possibili. Alla fine o abbiamo un goal (assegnamento ammissibile) oppure rispondiamo che il problema non è soddisfacibile (completezza e correttezza garantite).
\\ Usare DFS però non ci permette di sfruttare pienamente la struttura del problema. La search può essere rafforzata con un'attività di inferenza sui vincoli che si chiama \textbf{\textcolor{red}{constraint propagation}}, che funziona secondo questo principio:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/constraintPropagation.png}
\end{figure}
\subsection{Propagazione dei vincoli}
\subsubsection{Consistenza di nodo}
La prima e più semplice forma di propagazione dei vincoli si chiama \textbf{consistenza di nodo}. Si applica solo ai vincoli \textcolor{red}{unari} e, tipicamente, la si esegue in una fase di pre-processing del CSP.
\\ Procedimento: Per ogni variabile $X_i$ rimuovi da $D_i$ tutti i valori che non soddisfano i vincoli unari.
\\ \textcolor{ForestGreen}{\textbf{Esempio}: se ho vincolo $X_i \neq 5$, allora rimuovo $5$ dal suo dominio}.  
\\ Se alla fine di questo procedimento:
\begin{itemize}
    \item c'è almeno una variabile con dominio vuoto abbiamo risolto il CSP, che è \textcolor{red}{insoddisfacibile};
    \item ogni variabile ha un solo valore nel suo dominio: abbiamo risolto il CSP, facciamo l'unico assegnamento possibile e verifichiamo i vincoli;
    \item in tutti gli altri casi abbiamo un CSP equivalente, ma più compatto.
\end{itemize}
Raramente la consistenza di nodo risolve completamente un CSP.
\pagebreak
\subsubsection{Consistenza di arco}
La consistenza di arco propaga i \textbf{vincoli binari} (cioè gli archi del constraint graph).
 Una variabile $X_i$ è consistente rispetto a una variabile $X_j$ se per ogni valore in $D_i$ c'è un valore in $D_j$ che soddisfa il vincolo binario tra le due.
\\
\textcolor{ForestGreen}{\textbf{Esempio}:\\$X_1 \geq X_2,D_1=\{7,12\},d_2=\{1,2,15\} X_1$ arc-consistent rispetto a $X_2$ \\
$X_1 < X_2,D_1=\{7,6\},d_2=\{5,6,1\} X_1$ \textbf{non} arc-consistent rispetto a $X_2$.\\
Nel secondo esempio la \textbf{non} consistenza di arco ci suggerisce che 6 non può essere assegnato a $X_1$, quindi potremmo eliminarlo nel suo dominio $D_1$.}
\\ \textbf{Rendere una variabile arc-consistent}: eliminare dal suo dominio tutti i valori che la rendono non consistente rispetto a un'altra variabile con cui condivide un vincolo binario.
\\Un CSP è arc-consistent se tutti i nodi (le variabili) sono arc-consistent.
Valgono le stesse considerazioni fatte per la consistenza di nodo, ma in questo caso succede più spesso.
\subsubsection{Consistenza d'arco: pre-processing}
\begin{enumerate}
    \item Se non lo è, trasformare il constraint graph nella sua versione orientata (con archi)
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\textwidth]{Images/consistenza_darco.png}
    \end{figure}
    \item Esegui l'algoritmo AC-3 (Arc Consistency ver. 3).
\end{enumerate}
\subsubsection{AC-3}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Images/ac3.png}
\end{figure}
Una volta terminato il CSP risultato è equivalente a quello di partenza ma è arc-consistent. In generale potrebbe essere risolto prima.
\subsubsection{Path consistency}
Abbiamo visto come la consistenza d'arco in certi casi possa introdurre alcun vantaggio. La consistenza d'arco valuta le coppie di variabili. Possiamo generalizzare?\\
\textcolor{red}{\textbf{Consistenza di percorso}}: considera gruppi di 3 variabili.
\\ \textbf{Definizione}: due variabili $\{A,B\}$ sono path-consistent rispetto ad una variabile $C$ se per ogni assegnamento di $(A,B)$ consistente con i \textcolor{red}{vincoli tra $A$ e $B$}, esiste
un valore $C$ che sia:
\begin{itemize}
    \item consistente con i vincoli tra $A$ e $C$;
    \item cosistente con i vincoli tra $B$ e $C$.
\end{itemize}

\subsection{Algoritmi di Search per CSP}
In generale propagare i vincoli non implica la soluzione di un CSP, ma l'ottenimento di una semplificazione. Per costruire un assegnamento dobbiamo combinare la propagazione dei vincoli con una procedura di search.
\subsubsection{Backtracking Search}
\textcolor{red}{\textbf{Backtracking search}}: algoritmo di search per risolvere un CSP. 
\begin{itemize}
    \item \textbf{Nodo di partenza}: assegnamento vuoto $\Gamma$=$\{\}$.(tutte le variabili non sono assegnate).
    \item \textbf{Estensione del nodo}: assegnare ad una variabile $X_i$ una valore $v \in D_i$.
    \item Ricerca in profondità con backtracking.
    \item Nella costruzione di uno stato l'ordine degli assegnamenti non conto (trasposizioni): posso fissare un ordine e ad ogni livello di profondità assegno una variabile (profondità massima limitata dal numero di variabili).
    \item La propagazione dei vincoli viene fatta \textbf{durante} la ricerca per accelerarla.
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/backtracking_search.png}
\end{figure}
\subsubsection{Ordering di variabili e valori}
Le funzioni Seleziona-Variabile e Seleziona-Valore possono applicare un ordering su, rispettivamente, la prossima variabile e il prossimo valore da scegliere durante la ricerca.
\\ Euristica per le varibili:
\begin{itemize}
    \item Ordine lessicografico (statica).
    \item Random (dinamica).
    \item \textbf{Euristica di grado} (EG): scegliere la variabile che ha il maggior numero di vincoli con variabili correntemente non assegnate (dinamiche). \textcolor{Fuchsia}{Quella più difficile da posizionare, altrimenti se siamo in fondo dobbiamo tornare indietro tanto}
    \item  \textbf{Minimum Remaining Values} (MRV, "fail-first"): scegliere la variabile con il minor numero di valori possibili nel dominio (dinamica).
\end{itemize}
Nella pratica si usa EG per la prima variabile, poi MRV con EG per gli spareggi.
\\ Euristica per la scelta del valore:
\textbf{Least-Constraining Value} (LCV): scegliere il valore che esclude meno valori possibili per le variabili adiacenti (quelle che condividono un vincolo).
\pagebreak
\subsubsection{Metodi basati su ricerca locale}
Approccio alternativo alla risoluzione: non lavorare su assegnamenti parziali, ma su assegnamenti completi che possono violare uno o più vincoli. In un assegnamento $\Gamma_1$ che viola un certo 
numero di vincoli, cambio il valore di una variabile per ottenere un nuovo assegnamento $\Gamma_2$ che violi un numero \textbf{minore} di vincoli (iterative improvement).
Come scelgo la variabile e il nuovo valore da assegnare? Euristiche.
\\ Uno dei metodi più semplici da adottare è anche uno dei migliori: \textcolor{red}{\textbf{Min-Conflicts}}.
\\ Due principi di funzionamento:
\begin{enumerate}
    \item La scelta delle variabili è casuale.
    \item Il valore scelto è quello che \textbf{minimizza il numero di conflitti} nel nuovo assegnamento completo che si genera.
\end{enumerate}
\textcolor{red}{Non garantisce di trovare una soluzione e non è anytime.}
\\
Nella pratica Min-Conflicts funziona bene con CSP caratterizzati da elevati numeri di variabili. Questo succede perchè le istanze difficili sono tutte concentrate attorno ad un valore critico di $\rho$
definito come rapporto tra numero di vincoli e numero di variabili del problema.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/min_conflicts.png}
\end{figure}
\pagebreak
\section{Markov Decision Prodcess (MDP)}
Nuova classe di problemi più generale per un singolo agente. Gli ingredienti fondamentali sono:
\begin{itemize}
    \item abbandoniamo il concetto di goal come principale rappresentante dei desideri dell'agente;
    \item tempo orizzontale;
    \item ambiente \textbf{stocastico}, analogamente a quanto fatto nei problemi di adversarial search.
\end{itemize}
Sono utilizzati in svariate applicazioni tra cui la robotica.
\subsection{Formalizzazione}
\begin{itemize}
    \item $S=\{s,p,q,\dots\}$ spazio \textbf{finito} degli stati.
    \item $s_I \in S$ è lo stato \textbf{iniziale}, descrive la situazione di partenza in cui si trovano agente e ambiente.
    \item $s_T \in S$ è lo stato \textbf{terminale}:
    \begin{itemize}
        \item possono essere più di uno o anche nessuno;
        \item descrive la situazione in cui il processo di conclude, da uno stato terminale non si possono eseguire azioni.
    \end{itemize}
    \item Insieme \textbf{finito} di azioni $A(s)=\{a,b,c,\dots\}$.
    \item Modello di transizione $f$ \textbf{stocastico}:
    \begin{itemize}
        \item dato uno stato di partenza $s \in S$, un'azione $a \in A(s)$ e uno stato di arrivo $s'\in S$ indica la \textbf{probabilità della transizione}
        $ s \xrightarrow{a} s' $ e cioè la probabilità che eseguendo un'azione $a$ nello stato $s$ si passi allo stato $s'$;
        \item si rappresenta con una distribuzione di probabilità condizionata $P(s'|s,a)$.
    \end{itemize}
    \item  Ad ogni transizione $ s \xrightarrow{a} s' $ è associato un \textbf{reward} (ricompensa) numerico $R(s,a,s')$ che indica il \textbf{beneficio immediato} l'agente riceve un segnale di \textbf{reward additivo} $R(s,a,s')$ 
    (è un concetto equivalente a quello dei costi al netto di un'interpretazione diversa). Tutti con la stessa unità di misura.
    \item \textbf{Orizzonte} $H$ determina per quanti step (epoche di decisione) dura l'MDP: tutto quello che succede dopo $H$ transizioni di stato non importa, può essere $H = \infty$.
\end{itemize}
\subsection{Markovianità}
La Markovianità è una proprietà dei processi stocastici che prende il nome dal matematico russo Andrey Markov che la studiò nei primi anni del ventesimo secolo.
\\ Possiamo pensare ad un processo stacastico come ad una sequenza aleatoria di stati $s_0, s_1, s_2, \dots$ dove $s_t \in S$ è lo stato al tempo $t$. \\
Nel nostro modello tale sequenza è aleatoria perchè l'ambiente è stocastico, le azioni non hanno esiti certi ma descritti in termini probabilistici.
\\ \textbf{\textcolor{red}{Proprietà di Markov}}: l'esito di un'azione dipende solo dallo stato corrente (quindi non dagli stati e le azioni precedenti.)
\\ Nel mostro modello la proprietà è implicitamente codificata nel modello di transizione: $P(s'|s,a)$ infatti è un modo compatto di scrivere $P(s_{t+i}|s_t, a_t)$, dove $t$ è il tempo corrente.
\\ Il modello di transizione più generale possibile sarebbe indicato come $P(s_{t+1} | s_t, a_t, s_{t-1}, \dots, s_0,a_0)$ ma nel nostro modello tutti gli stati e le azioni intraprese nel passato non influiscono sullo stato futuro
e quindi possono essere cancellate.
\pagebreak 
\subsection{Soluzione di un MDP}
Nei problemi di search cercavamo un \textbf{piano} (nel caso in cui il problema fosse un CSP il piano corrisponde ad un assegnamento di variabili eseguito in qualsiasi ordine). Nei problemi di adversial search cercavamo una \textbf{strategia}.
La soluzione di un MDP si chiama \textbf{\textcolor{red}{policy}} ed è definita come una funzione che, dato uno stato, prescrive quale azione l'agente deve compiere:
\[ \pi: S \rightarrow A \] 
La policy è deterministica (non randomizza mai su quale azione prescrivere), ma la sequenza di stati che una policy genera è stocastica per via degli esiti incerti delle azioni.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Images/MDP.png}
\end{figure}
Se ci viene fornita una policy $\pi$ per un particolare MDP, l'agente che la implementa esegue iteratamente questi step:
\begin{enumerate}
    \item Osserva lo stato in cui si trova, detto \textbf{stato corrente} $s$.
    \item Consulta la policy $\pi$ e ottiene l'azione $pi(s)$ che essa prescrive.
    \item Esegue l'azione $\phi(s)$ e ritorno in 1.
\end{enumerate}
È un agente reflex. Eseguire e definire una policy è facile, mentre ci serve definire un metodo per definire una policy che massimizzi un critierio di qualità.
\\ Come si misura la bontà di una policy? Possiamo riusare lo stesso criterio che, nei problemi di search, abbiamo adottato per cercare un piano ottimo. La sequenza di azioni che consente di raggiungere il goal e accumula,
lungo il percorso, il minor costo possibile.
\textbf{Problemi}: Non abbiamo più stati goal e non abbiamo più costi, ma reward che, per di più, sono soggetti ad incertezza.
\\ \textbf{\textcolor{red}{Idea}}: massimizzare il valore atteso della somma dei reward che si ottengono eseguendo la policy $\pi$.
\\ La policy $\pi$ genera una qualche sequenza di stati $s_0, s_1, s_2, \dots$ che a sua volta corrisponde ad una sequenza di reward $R_0, R_1, R_2, \dots$. Maggiore è la somma
$U(s_0,s_1, \dots) = R_0,R_1,R_2, \dots $ migliore è la sequenza di stati che si è percorsa.
\\ La policy che cerchiamo deve massimizzare il \textbf{valore atteso} della somma dei reward rispetto alle sequenze si stati che può generare. La chiameremo policy \textbf{ottima} $\pi^*S \rightarrow A$, ogni altra policy $\pi$ genererà un reward atteso inferiore o uguale
\[
\pi^* = \mathop{\arg\max}_{\pi} \, \mathbb{E}_{\pi}\big[\, U(s_0, s_1, \ldots ) \,\big]
\]
\subsection{Valore atteso della somma dei reward}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Images/reward1.png}
    \includegraphics[width=1\textwidth]{Images/reward2.png}
\end{figure}
\subsection{Reward scontati}
La somma può avere infiniti termini quando $H=\infty$, come compariamo due policy che hanno reward totale infinito? Per fare ciò consideriamo una \textbf{somma scontata}.
\\ Nella somma dei reward è ragionevole modellare il fatto che l'agente dia più importanza ai reward più imminenti rispetti a quelli molto lontani nel futuro.
\\ \textbf{\textcolor{ForestGreen}{Metodo}}: applicare un decadimento esponenziale ai termini della somma dei reward:
\[ U(s_0,s_1,,\dots)= \gamma^0 R_0 + \gamma^1 R_1 + \gamma^2 R_2 + \dots = \sum_{t=0}^{H} \gamma^t R_t \]
Dove $0<\gamma<1$ è il \textbf{fattore di sconto}, più è piccolo più l'agente è "impaziente".
\\ Se $H=\infty, \sum^{\infty}_{t=0} \gamma^t R_t \leq R_{max} \sum^{\infty}_{t=0} \gamma^t = \frac{R_{max}}{1-\gamma}$, anche se ci sono infiniti termini, la somma è finita.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Images/rewardScontati.png}
\end{figure}
\subsection{Orizzonte finito o infinito?}
Potremmo pensare che un orizzonte \textbf{finito} sia una situazione più conveniente:
\begin{itemize}
    \item il processo a un certo punto si ferma, come in una ricerca in profondità dove non si va mai oltre una profondità massima.
    \item Le somme dei reward hanno un numero finito di termini: $U(s_0,s_1,\dots, s_H)=\sum^H_{t=0}R_t$.
\end{itemize}
Con orizzonte finito però si presenta un grosso svantaggio: la policy ottima (quella che massimizza il valore atteso di $U()$) \textbf{non è stazionaria}. Significa che l'azione
da compiere cambia a seconda del momento in cui visitiamo un certo stato.
\\ Lo stato da solo non basta, devo considerare anche il tempo: perdiamo la buona proprietà indotta dalla Markovianità nel nostro MDP.
\subsubsection{Stazionarietà della policy ottima}
Se l'orizzonte $H$ ha un valore finito, significa che ogni volta che l'agente visita uno stato c'è un tempo rimanente $h-T$ che diminuisce transizione dopo transizione.
\\ Supponiamo di trovarci, dopo $t$ transizioni, nello stato $c$
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Images/stazionarieta.png}
\end{figure}
Che cosa succede nell'esempio se diciamo all'agente che ha sempre di fronte a sé un tempo infinito?
\textbf{\textcolor{ForestGreen}{Stazionarietà della policy ottima}}: la cosa migliore da fare è funzione unicamente allo stato.
\pagebreak
\subsection{Riduzione di un MDP}
Nella pratica adotteremo sempre un \textbf{orizzonte infinito} e utilizzeremo un \textbf{fattore di sconto} $\gamma$ per evitare di avere sempre somme di reward infinite.
\\ Come si calcola la policy ottima $\pi^*:S \rightarrow A$?
\\ \textbf{Dynamic programming}: metodo di ottimizzazione che scompone il problema di partenza $P$ in uno o più sotto-problemi $P_1,P_2,\dots$; la soluzione $P$ è ottenuta combinando soluzioni di $P_1, P_2, \dots$
a loro volta scomposti in modo ricorsivo.
Questo metodo si applica a quei problemi dotati di sotto-scrittura ottima o, alternativamente, per cui vale il \textbf{principio di ottimalità di Bellman}.
\\ Un problema $P$ soddisfa il principio di Bellman se nella soluzione ottima di $P$ le soluzioni dei sotto-problemi $P_1, P_2, \dots$ sono esse stesse soluzioni ottime 
per quei sottoproblemi. È un principio fondamentale che caratterizza e permette di risolvere un elevato numero di problemi.
\subsection{Principio di ottimalità}
Nell'MDP il principio di ottimalità è soddisfatto. Per poterlo "dare" forma alla sotto-struttura ottima dobbiamo introdurre queste nuove quantità che descrivono/definiscono la policy ottima $\pi^*$.
\\ \textcolor{red}{$V^*(S)$} valore (ottimo) dello stato $s$ (anche detta \textit{state-value function}):
\begin{itemize}
    \item L'agente si trova in $s$.
    \item Se \textbf{da lì in avanti} seguisse la policy ottima $\pi^*$ quanto si aspetta di ottenre in termini di somma totale di reward?
    \item Dipende dallo stato perchè la policy ottima è stazionaria.
\end{itemize}
\textcolor{blue}{$Q^*(s,a)$} valore (ottimo) della coppia stato/azione $s,a$ (anche detta \textit{action-value function}):
\begin{itemize}
    \item L'agente si trova in $s$ e ha deciso di seguire $a$.
    \item Se in \textbf{tutte le scelte successive} seguisse la polici ottima $\pi^*$ quanto si aspetta di ottenre in termini di somma totale di reward?
    \item Lo stesso di $V$ ma abbiamo "scorporato" la prima decisione.
\end{itemize}
I due valori, \textcolor{blue}{$Q^*$} e \textcolor{red}{$V^*$}, sono legati tra loro. Se l'agente si trova in $s$ la policy ottima prescriverà l'azione che massimizza \textcolor{blue}{$Q^*(s,a)$}, quindi
\[ V^*(s) = \max_{a} Q^*(s,a) \]
Conoscere $V^*()$ implica di conoscere la policy ottima, quindi implica di aver risolto l'MDP. Che espressione ha \textcolor{blue}{$Q^*$}?
\begin{itemize}
    \item Visto che l'agente esegue l'azione $a$ nello stato $s$, otterrà un reward immediato che dipende dallo stato di arrivo $s'$ in cui l'azione $a$ lo porta (stocastico).
    \item Visto che da $s'$ in avanti l'agente seguirà la policy ottima, al reward immediato aggiungerà $V*(s')$.
\end{itemize}
\[ Q^*(s,a) = \sum_{s'} P(s'|s,a) [ R(s,a,s') + \gamma V^*(s') ] \]
P è la probabilità della transizone $s \xrightarrow{a} s'$, R è il reward a fronte della transizione $ s \xrightarrow{a} s' $ e V è il reward futuro scontato da $s'$ in avanti con policy ottima.
\\ Combinando le due uguaglianze otteniamo un'espressione per il valore ottimo dello stato: 
\[ V^*(s) = \max_{a} \sum_{s'} P(s'|s,a) [ R(s,a,s') + \gamma V^*(s') ] \]
\textbf{\textcolor{red}{Principio di ottimalità di Bellaman}}: la soluzione ottima per uno stato $s$ ha questa forma:
\begin{enumerate}
    \item Fare un'azione (che va scelta in modo ottimale).
    \item Applicare la soluzione ottima per lo stato di arrivo.
\end{enumerate}
L'espressione che abbiamo derivato è la celebre \textbf{equazione di Bellman}. Ammette un'\textbf{unica soluzione}: $V^*()$, la state-value function indotta dalla policy ottima.
\\L'equazione di Bellman ci mostra la sott-struttura ottima del problema. Possiamo applicare metodi basati su \textbf{Dynamic Programming}: scomposizione in sotto-problemi e risoluzione ricorsiva.
\\L'equazione non solo ci indica questa possibilità ma ci suggerisce anche un metodo operativo con cui procedere.
\\ Vedremo i due approcci risolutivi più famosi basati su Dynamic Programming:
\begin{itemize}
    \item \textbf{\textcolor{red}{Value Iteration}}
    \item \textbf{\textcolor{blue}{Policy Iteration}}
\end{itemize}
\subsection{Running example}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Images/runningExample.png}
\end{figure}
\subsection{Value Iteration}
Metodo: costruire iterativamente i valori di $V^*$.
\\ Inizializzo $V_0(s)=0$ per ogni stato $s$ e ad ogni iterazione $K+1$ calcolo $V_{k+1}(s)$ per ogni stato $s$ usando i valori $V_k(s)$ ottenuti nell'iterazione precedente. Lo si fa tramite \textbf{Bellman Update}
\[ V_{k+1}(s) \leftarrow  \max_{a} \sum_{s'} P(s'|s,a) [ R(s,a,s') + \gamma V_k(s') ] \]
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Images/valueIteration.png}
\end{figure}
Calcolo di $Q(s,a)$ lavora con i valori di $V$ dell'iterazione precedente. Il test di convergenza verifica se i nuovi valori $V_{k+1}(s)$ sono "praticamente" uguali a quelli precedenti:
\begin{itemize}
    \item \textbf{somma delle differenze}: $\sum_s |V_{k+1}(s) - V_k(s)| \leq \epsilon$;
    \item \textbf{span}: $|\max_{s}(V_{k+1}(s)-V_k(s)) - \min_{s} (V_{k+1}(s)-V_k(s))| \leq \epsilon$.
\end{itemize}
\textbf{\textcolor{ForestGreen}{Proprietà fondamentale}}: iterazione dopo iterazione i valori $V_k(s)$ convergono verso $V^*(s)$.
\pagebreak
\subsubsection{Convergenza di Value Iteration}
Intuizione sul perchè Value Iteration converge verso valori ottimi.
Osserviamo il Bellman Update e riflettiamo sulla sua definizione, che cosa ci ricorda?
\[ V_{k+1}(s) \leftarrow  \max_{a} \sum_{s'} P(s'|s,a) [ R(s,a,s') + \gamma V_k(s') ] \]
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Images/giocoStocastico.png} 
\end{figure}
Le sequenze di applicazione del Bellman Update per uno stato $s$ possono essere rappresentate con lo stesso formalismo di un gioco stocastico in cui c'è solo il giocatore MAX
\[ V_{k+1}(s) \leftarrow  \max_{a} \sum_{s'} P(s'|s,a) [ R(s,a,s') + \gamma V_k(s') ] \]
\textcolor{ForestGreen}{Esempio con $k=0$}
\begin{figure}[H]
    \includegraphics[width=0.3\textwidth]{Images/legenda.png}
    \\ 
    \includegraphics[width=1\textwidth]{Images/convergenzak0.png}
\end{figure}
\textcolor{ForestGreen}{Esempio con $k=1$}
\begin{figure}[H]
    \includegraphics[width=1\textwidth]{Images/convergenzak1.png}
\end{figure}
La struttura dell'albero \textcolor{ForestGreen}{$k$} è contenuta in quella dell'albero \textcolor{blue}{$k+1$}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Images/convergenzaUnita.png}
\end{figure}
Che relazione c'è tra $V_{k+1}(s)$ e $V_k(s)$?
\\ Dipende da quello che trovo nel calcolo di $V_{k+1}(s)$ \textcolor{red}{al livello di profondità $k+1$} (quello esplorato in più rispetto a $V_k(s)$).
\\ I reward che si ottengo sulle $k+1$-esime transizioni devono essere tutti minori o uguali a $R_{MAX}$ (il massimo reward possibile nell'MDP) e maggiori o uguali a $R_{MIN}$
(il minimo reward possibile nell'MDP).
\\ Caso migliore: sono tutti uguali a $R_{MAX}$, quindi ogni transizione al livello ${k+1}$ aggiunge $\gamma^kR_{MAX}$.
\\ Caso peggiore: sono tutti uguali a $R_{MIN}$, quindi ogni transizione al livello ${k+1}$ aggiunge $\gamma^kR_{MIN}$.
\\Quindi: $V_k(s) + \gamma^k R_{MIN} \leq V_{k+1}(s) \leq V_k(s) + \gamma^k R_{MAX}$
\\ Essendo $0\leq \gamma <1$, per $k \rightarrow \infty$ si ha $V_k(s)=V_{k+1}(s)$, \textbf{Value Iteration converge!}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Images/valueIterationRis.png}
\end{figure}
\subsection{Policy Extraction}
Conoscere $V^*()$ è equivalente a conoscere $\pi^*$, ma nella pratica come possiamo sintetizzare la policy ottima a partire dalla state-value function?
\\ \textbf{\textcolor{blue}{Policy Extraction}}: calcolo della policy ottima a partire dalla state-value function ottima
\[ \pi^*(s) = \mathop{\arg\max}_{a} \sum_{s'} P(s'|s,a) [ R(s,a,s') + \gamma V^*(s') ] \]
Applicato al risultato della value iteration nell'esempio precedente otteniamo $\pi^*(sf)=H$, $\pi^*(ex)=L$
\subsection{Value Iteration: svantaggi}
È un procedimento abbastanza \textbf{inefficiente}: ad ogni iterazione, nel caso pessimo, dobbiamo calcolare $|S| \times |A| \times |S|$ termini, e come si sceglie $\epsilon$? Molti
di questi termini si ripetono, c'è un sacco di lavoro ridondante (anche se si possono implementare delle look-up tables, simili alle transposition tables).
\\ La policy di norma \textbf{\textcolor{blue}{converge prima della state-value function}}. Se applichiamo Policy Extraction ad ogni iterazione nell'esempio precedente:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Images/valueIterationSvantaggi.png}
\end{figure}
\subsection{Policy Iteration}
Per provare a mitigare questi problemi esiste un metodo alternativo chiamato \textbf{\textcolor{red}{policy iteration}}.
Si lavora direttamente sulla policy e non sulla state-value function. L'approccui su basa su questi step:
\begin{enumerate}
    \item scegliere una policy iniziale $\pi$ (casuale);
    \item \textbf{\textcolor{Fuchsia}{Policy evaluation}}: calcolare i valori della state-value function $V^\pi(s)$ che la policy $\pi$ induce nell'MDP (\textcolor{red}{Attenzione!} Non saranno i valori ottimi!);
    \item \textbf{Policy Extraction} usando i valori $V^\pi(s)$ appena calcolati, si estrae una nuova policy $\pi'$;
    \item la nuova policy $\pi'$ è uguale alla vecchia $\pi$?
    \begin{enumerate}
        \item Se sì l'algoritmo termina e restituisce $\pi$
        \item Altrimenti \textbf{\textcolor{red}{policy update}}: sostituisci $\pi$ con $\pi'$ e ritorna allo step 2.
    \end{enumerate} 
\end{enumerate}
\subsection{Policy evaluation}
Problema inverso rispetto alla policy extraction: ho una policy data $\pi$ (non necessariamente ottima) e la devo valutare: calcolare i valori $V^\pi(s)$ che $\pi$ garantisce.
Visto che la policy è data, la scelta di quale azione fare in ogni stato è fissata e nota a priori, quindi
\[ V^\pi(s) = \sum_{s'} P(s'|s,\pi(s)) [ R(s,\pi(s),s') + \gamma V^\pi(s') ] \]
Come si risolve? Due alternative:
\begin{itemize}
    \item Applichiamo un procedimento di \textbf{value iteration} con Bellman update modificato dove, non essendoci più il max e l'azione è fissata dalla policy $\pi$, si ha: $V^\pi_{k+1}(s) \leftarrow Q(s,\pi(s))$ dove $Q(s,\pi(s))$ è calcolato 
    usando i valori $V^\pi_k(s)$ dall'iterazione precedente.
    \item Visto che non c'è il max l'equazione è \textbf{lineare}: associo una variabile ad ogni $V^\pi(s)$ e ottengo un sistema lineare di $|S|$ equazioni in $|S|$ incognite.
    \item Potremmo usare anche Expectimax.
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Images/policyIteration.png}
\end{figure}
\subsection{Recap}
Value e policy iteration sono algorimti di programmazione dinamica per il calcolo della policy ottima in un MDP.
\\ \textbf{\textcolor{red}{Value iteration}}
\begin{itemize}
    \item Lavora sulla state-value function, la policy è trattata indirettamente attraverso l'operatore max nel Bellman update.
    \item Una volta terminato bisogna fare policy extraction.
\end{itemize}
\textbf{\textcolor{blue}{Policy iteration}}
\begin{itemize}
    \item Lavora direttamente sulla policy, migliorandola iterativamente.
    \item In ogni iterazione svolge una policy evaluation della policy incombente.
    \item Di norma converge prima di value iteration.
\end{itemize}
\textbf{\textcolor{red}{Value iteration}}: metodo di programmazione dinamica per calcolare la state-value function $V^*()$ della policy ottima $\pi^*$ in un MDP, lavora aggiornando iterativamente i valori della funzione.
\\ \textbf{\textcolor{blue}{Policy iteration}}: metodo di programmazione dinamica per calcolare la policy ottima $\pi^*$ in un MDP, lavora aggiornando iterativamente la policy.
\\ \textbf{\textcolor{Cerulean}{Policy evaluation}}: data una policy $\pi$ calcolare la state-value function $V^\pi()$ che questa genera in un MDP, viene usata in uno step di policy iteration.
\\ \textbf{\textcolor{Fuchsia}{Policy extraction}}: data una state-value function $V^\pi()$,calcolare la policy $\pi$ che la genera, viene usata alla fine di value iteration per ottenere la policy ottima e in uno step di policy iteration.
\\ \textbf{\textcolor{ForestGreen}{Policy update}}: modifica una policy $\pi$ per ottenre una nuova $\pi'$ che di solito è migliore, viene usata in uno step di policy iteration.
\pagebreak
\section{Reinforcement Learning}
Sappiamo che un MDP è dato dalla tupla $\langle S,A,P,R \rangle$, che fornisce insieme degli stati $S=\{s,q,p,\dots\}$, azioni $A(s)$, modello di transizione $P(s' | s,a)$ e funzione di reward $R(s,a,s')$.
\\ Rappresentazione compatta: $\langle P,R \rangle$, conoscere modello di transizione e funzione di reward significa, implicitamente, conoscere tutto l'MDP.
\\ $\langle P,R \rangle$, il nostro MDP, è anche detto \textbf{modello}: codifica le leggi con cui l'ambiente risponde alle azioni dell'agente in termini di transizioni di stato (effetti di un'azione) e di reward (bontà di una transizione).
\\ Fino ad ora il modello è assunto come conosciuto e il problema affrontato, quindi, è: dato un modello $\langle P,R \rangle$ come trovare la policy ottima per quel modello? \textbf{\textcolor{ForestGreen}{Risposta}} \textcolor{ForestGreen}{: dynamic programming}.
\\ \textbf{\textcolor{red}{Nuovo problema}}: come trovare la policy ottima in situazioni in cui il modello $\langle P,R \rangle$ è \textcolor{red}{inizialmente sconosciuto}? Tramite il \textbf{reinforcement learning}.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Images/reinforcementLearning.png}
\end{figure}
Nello scenario di modello noto eseguire un'azione è il mezzo con cui l'agente svolge il proprio compito nell'ambiente, cercando di massimizzare la performance. Nel nuovo scenario di modello sconosciuto, eseguire un'azione ha una duplice funzione
\begin{itemize}
    \item svolgere il compito;
    \item rivelare le leggi del modello.
\end{itemize}
\textbf{Come si sceglie l'azione?} Due modi:
\begin{itemize}
    \item \textcolor{blue}{Exploration}: scegliere un'azione privilegiando l'acquisizione di nuove informazioni del modello (sbaglio ma imparo).
    \item \textcolor{red}{Exploitation}: scegliere un'azione privilegiando le performance rispetto a quanto si è appreso del modello (cerco di non sbagliare al netto di quello che conosco).
\end{itemize}
In generale, il modello appreso \textbf{non è perfettamente uguale} a quello reale ma ne rappresenta una stima. Fare la cosa ottima (exploitation) nel modello appreso non sarà buona come fare la cosa ottima nel modello reale.
La differenza di utilità tra i due casi si chiama \textbf{regret} (rimorso).
\pagebreak
\subsection{Passive RL}
Consideriamo una prima versione del problema che il nostro agente deve affrontare: \textbf{Reinforcement Learning Passivo}. In Passive RL si assume che una policy $\pi$ sia data e fissata.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Images/passiveRL.png}
\end{figure}
\textbf{Problema affrontato dalla agente}: imparare quanto è buona la policy $\pi$. 
\\ In pratica imparare $V^\pi(s)$ dove $s$ è un qualsiasi stato raggiungibile eseguendo $\pi$ per un numero finito di step, assumendo un fattore di sconto $\gamma$. Stesso problema che abbiamo precedentemente chiamato policy evaluation, ma ora
$P(s'|s,a)$ e $R(s,a,s')$ sono inizialmente sconosciuti. L'agente non deve calcolare una policy ottima, ma solo scoprire quanto è buona una policy data.
\subsection{Adaptive Dynamic Programming}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Images/adaptiveDynamicProgramming.png}
\end{figure}
\textbf{\textcolor{YellowGreen}{Esplorazione}}: genera un dataset (training set) fatto da un certo numero di episodi.
\textbf{Episodio}: una sequenza di transizioni e reward osservati di una lunghezza massima fissata (in caso si finisca in uno stato terminale si conclude l'episodio indipendentemente dalla sua lunghezza). Ogni transizione ha questa forma:
$s_1 \xrightarrow{R(s_1, \pi(s_1),s_2)} s_2$ 
\\ \textbf{\textcolor{Apricot}{Stima del modello}}: per ogni stato osservato ed azione eseguita durante l'esplorazione si calcolano:
\begin{itemize}
    \item Numero di volte in cui l'agente è stato nello stato $s$ e ha svolto l'azione $a$: $\#(s,a)$.
     \item Numero di volte in cui l'agente è stato nello stato $s$, ha svolto l'azione $a$ e si è ritrovato nello stato $s'$: $\#(s,a,s')$.
\end{itemize}
Queste quantità dipendono dalla policy $\pi$ e dal modello delle \textcolor{red}{transizioni} (sconosciuto) che determina probabilisticamente gli esiti delle azioni intraprese.
\\ Stima del modello di tranzione basata sulla frequenza relativa empirica di ogni stato osservato:
\[ \hat{P}(s'|s,a) = \frac{\#(s,a,s')}{\#(s,a)} \]
\\ Scoperta di alcuni reward: ogni transizione ha rivelato il reward associato a quella transizione: 
\[ \hat{R}(s,a,s') = R(s,a,s') \]
Una volta costruite $\hat{P}$ e $\hat{R}$ l'agente assume di trovarsi in un MDP il cui modello sia $\langle \hat{P}, \hat{R} \rangle$ (stati/azioni saranno quelli
che sono stati osservati/eseguiti almeno una volta durante l'esplorazione).
\\ Viene eseguita la \textbf{\textcolor{SkyBlue}{policy evaluation}} di $\pi$ dentro l'MDP $\langle \hat{P}, \hat{R} \rangle$
\[ V^\pi(s) = Q(s,\pi(s))= \sum_{s'} \hat{P}(s'|s,\pi(s)) [ \hat{R}(s,\pi(s),s') + \gamma V^\pi(s') ] \]
Vengono usati i metodi che  già conosciamo, in alternativa:
\begin{itemize}
    \item Risoluzione di un sistema di equazioni lineari.
    \item Dynamic Programming (value iteration fino a convergenza).
\end{itemize}
\subsubsection{Esempio ADP}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Images/esempioADP.png}
\end{figure}
\textcolor{Fuchsia}{Quante volte mi sno trovato nello stato sf e ho fatto l'azione H?12 volte. Di queste 12 volte quante volte facendo l'azione H essendo nello stato sf, sono rimasto in sf? 9 volte
Il modello trovato è diverso da quello reale: vediamo che manco una uno stato (quello finale, molto importante). Quindi la policy ottima di questo MDP differisce da quella reale.
}
\subsection{Adaptive Dynamic Programming}
Cerca di apprendere il modello per utilizzare le tecniche che lavorano sotto l'assunzione di conoscere il modello. Per questo motivo l'ADP è detto \textbf{\textcolor{blue}{Model-based}}.
\\ Aumentando il numero di episodi nel training set la stima del modello \textbf{migliora}: le frequenze empiriche che emergono dalle osservazioni convergono verso le vere probabilità di transizione.
Migliore è la stima del modello, più realistica è la valutazione della policy.
\\ Quando l'MDP sottostante ha un numero \textbf{molto grande} di stati può richiedere un numero molto alto di episodi, l'esplorazione richiede troppo tempo per poter fornire una stima ragionevole del modello.
Deve mantenere il conto totale di visite e transizioni, richiedendo però molta memoria.
\\ Un approccio alternativo è il \textbf{\textcolor{red}{Model-Free}}: non stimare il modello, ma direttamente i valori della state value function.
\subsection{Stima diretta}
È un metodo più semplice per stimare $V^\pi()$ senza costruire $\hat{P}$ e $\hat{R}$: si genera un dataset con una fase di esplorazione uguale a quella di ADP. Per ogni visita allo stato $s$ estraggo il reward accomulato da $s$ fino alla fine dell'episodio.

\[TotR_1^s,TotR_2^s,TotR_3^s, \dots, TotR_k^s\]

Divido la somma dei totali per il numero di visite ad $s$ (media aritmetica):
\[\hat{V}^\pi(s)=\frac{1}{k}\sum_{i=1}^k TotR_i^s\]
Anche in questo caso più prolungata è l'esplorazione (maggior numero di episodi nel dataset, maggior numero di visite ad $s$) più $\hat{V}^\pi(s)$ tende a $V^\pi(s)$.
\\ \textcolor{ForestGreen}{Vantaggi}: semplicità
\\ \textcolor{red}{Svantaggi}:  
\begin{itemize}
    \item Richiede comunque di tenere traccia del numero di visite ad ogni stato osservato.
    \item Stima i vari $V^\pi(s)$ come se fossero indipendenti tra di loro, anche se c'è una dipendenza che include il modello di transizione e prende la forma dall'equazione di Bellman.
\end{itemize}
\subsection{Temporal Difference Learning}
I limiti di base della stima diretta sono risolvibili con un secondo approccio di tipo Model-Free chiamato \textbf{Temporal Difference Learning} (TD).
\\ \textcolor{ForestGreen}{\textbf{Idea}}: aggiornare le stime di $V^\pi()$ in modo iterativo ad ogni transizione effettuata in fase di esplorazione.
\\  L'agente si trova nello stato $s$ per il quale ha calcolato la stima corrente $\hat{V}^\pi(s)$ (la stima corrente è calcolata per ogni stato osservato). Esegue l'azione $\pi(s)$ osservando la transizione $s \xrightarrow{R} s'$:
\begin{itemize}
    \item il fatto che l'azione prescritta dalla policy nello stato $s$ sia risultata in $s'$ con reward $R$ può essere usato per migliorare la stima corrente $\hat{V}^\pi(s)$.
    \item In particolare, immaginando di considerare unicamente la transazione osservata (come se fosse l'unica possibile da $s$) si può affermare:
\end{itemize}
\[\hat{V}^\pi(s)=R+\gamma \hat{V}^\pi(s')\]
$R+\gamma \hat{V}^\pi(s')$ è quanto dovrebbe valore lo stato $\hat{V}^\pi(s)$ in base alla \textbf{sola} transizione che è stata osservata, chiamiamo questa quantità \textbf{$z$}, viene calcolata \textbf{dopo} aver fatto la transiozione usando il reward ottenuto e la stima
corrente della state-value function nello stato di arrivo.
\\ In generale, $z$ sarà diverso dalla stima corrente $\hat{V}^\pi(s)$ che avevamo prima della transizione.
\\ \textbf{Update} di $\hat{V}^\pi(s)$ sulla base dello $z$ calcolato dalla transizione osservata:
\[\hat{V}^\pi(s) \leftarrow \hat{V}^\pi(s)+ \textcolor{blue}{\alpha}(z - \hat{V}^\pi(s)) \]
dove $0\leq \alpha \leq 1$ è un parametro detto \textbf{\textcolor{blue}{learning rate}}
\\ La stima $\hat{V}^\pi(s)$ viene "corretta" sulla base dell'osservazione $z$ in modo da avvicinarla a quanto è stato suggerito dall'esperienza: il termine $z - \hat{V}^\pi(s)$ è un segnale di errore.\\
La learning rate quantifica la "forza" di tale correzione:

\begin{itemize}
    \item $\alpha =0$, la transizione osservata viene \textbf{ignorata} e la stima non viene corretta;
    \item $\alpha=1$, la vecchia stima viene \textbf{rimpiazzata} con quanto indicato dall'osservazione $z$;
\end{itemize}
Sostituiendo $z$ e semplificando abbiamo:
\[\hat{V}^\pi(s) \leftarrow (1-\alpha)\hat{V}^\pi(s) + \alpha ( R(s,a,s') + \gamma \hat{V}^\pi(s') )\]

La nuova stima di $\hat{V}^\pi(s)$ è una combinazione convessa tra la vecchia stima e quella derivata dall'osservazione: un po' dell'una e un po' dell'altra, a seconda di $\alpha$.
\\ Immaginando di ripetere iterativamente questo update sulla base di una sequenza di osservazioni $z_1,z_2,z_3,\dots, z_k$ fatte a partire dallo stato $s$:
\begin{itemize}
    \item ad ogni iterazione otterremo una nuova versione di $\hat{V}^\pi(s)$ (similmente a quello che succedeva con Value Iteration);
    \item chiamiamo $\hat{V}_i^\pi(s)$ la stima corretta ottenuta considerando l'osservazione $z_i=R(s,\pi(s),s')+\gamma \hat{V}_{i-1}^\pi(s')$;
\end{itemize}
\[
\hat{V}^\pi_k(s)
  = \textcolor{blue}{(1-\alpha)}\,\textcolor{red}{\hat{V}^\pi_{k-1}(s)}
  + \textcolor{blue}{\alpha z_k}
\]

\[
\hat{V}^\pi_k(s)
  = \textcolor{blue}{(1-\alpha)}
    \Big[
      \textcolor{red}{(1-\alpha)}\,\textcolor{green!60!black}{\hat{V}^\pi_{k-2}(s)}
      + \textcolor{red}{\alpha z_{k-1}} 
    \Big]
    + \textcolor{blue}{\alpha z_k}
\]

\[
\hat{V}^\pi_k(s)
  = \textcolor{blue}{(1-\alpha)}
    \Big[
      \textcolor{red}{(1-\alpha)}
      \Big[
        \textcolor{green!60!black}{(1-\alpha)}\,\textcolor{purple}{\hat{V}^\pi_{k-3}(s)}
        + \textcolor{green!60!black}{\alpha z_{k-2}}
      \Big]
      + \textcolor{red}{\alpha z_{k-1}}
    \Big]
    + \textcolor{blue}{\alpha z_k}
\]

\[
\hat{V}^\pi_k(s)
  = \textcolor{blue}{(1-\alpha)}
    \Big[
      \textcolor{red}{(1-\alpha)}
      \Big[
        \textcolor{green!60!black}{(1-\alpha)}
        \Big[
          \textcolor{purple}{(1-\alpha)}\,\hat{V}^\pi_{k-4}(s)
          + \textcolor{purple}{\alpha z_{k-3}}
        \Big]
        + \textcolor{green!60!black}{\alpha z_{k-2}}
      \Big]
      + \textcolor{red}{\alpha z_{k-1}}
    \Big]
    + \textcolor{blue}{\alpha z_k}
\]

\[
\vdots
\]

\[
\hat{V}^\pi_k(s)
  = \alpha \sum_{i=0}^{k-1} (1-\alpha)^i\, z_{k-i}
    + (1-\alpha)^k \hat{V}^\pi_0(s)
\]
$V_0^\pi(s)$ è la stima di partenza che possiamo dare in modo arbitrario, per semplificare la scegliamo pari a 0. 
\textcolor{Fuchsia}{Più una cosa che ho osservato è vecchia, meno deve contare nel valore corrente della stima. Le osservazioni recenti pesano di più rispetto a quelle "antiche
}
\[\hat{V}^\pi_k(s)
  = \alpha \sum_{i=0}^{k-1}(1-\alpha)^i\, z_{k-i}\]
\textcolor{red}{Media "mobile" esponenziale}: più è grande $i$ più esponenzialmente piccolo è il peso dell'elemento associato nella media $(1-\alpha \leq 1)$.
\\ Le osservazioni più vecchie hanno un peso esponenzialmente più piccolo della stima corrente.
\\ \textbf{Il principio è pertinente con il processo di stima}: le osservazioni più vecchie sono basate su versioni più vecchie di $\hat{V}^\pi(s)$ e quindi meno corrette (si ricordi che $z_j=R(s,\pi(s),s')+\gamma \hat{V}^\pi_{j-1}(s'),j\leq 1$).
\\ L'algoritmo può essere schematizzato così: 
\begin{enumerate}
    \item inizializza $\hat{V}^\pi(s)=0$ per lo stato iniziale $s_0$;
    \item osserva lo stato corrente  $s$, \textcolor{ForestGreen}{esegue l'azione $\pi(s)$} e riceve/osserva un reward $R$ e uno stato di arrivo $s'$; se è la prima volta che visita $s'$ inizializza $\hat{V}^\pi(s')=0$;
    \item calcola $z=R+\gamma \hat{V}^\pi(s')$;
    \item \textbf{\textcolor{red}{update}}: $\hat{V}^\pi(s) \leftarrow \hat{V}^\pi(s)+ \alpha (z - \hat{V}^\pi(s))$;
    \item riaparti da 2;
\end{enumerate} 
\textbf{Domanda}: l'algoritmo converge verso $V^\pi()$  (la state-value function corretta indotta dalla policy $\pi$)?
\\ \textbf{\textcolor{ForestGreen}{Risposta}}: sì a patto di adattare la learning rate man mano che si procede:
\begin{itemize}
    \item la learning rate $\alpha$ diventa una funzione $\alpha(N_s)$ dove $N_s$ è il numero di volte che l'agente ha visitato $s$;
    \item per ottenere la convergenza deve valere: $\sum_{n=1}^\infty \alpha(n) = \infty$ , $\sum_{n=1}^\infty \alpha^2(n) < \infty$, nella pratica si adotta una funzione decrescente con rate $O(\frac{1}{n})$.
\end{itemize}
\subsection{Active RL}
Versione più interessante del problema di \textbf{Reinforcement Learning Attivo}. La policy \textbf{non è data}, l'agente la deve calcolare.
\\ \textbf{Problema affrontato dall'agente}: calcolare la policy ottima $\pi^*$. Due livelli di difficoltà concorrenti:
\begin{itemize}
    \item Capire le dinamiche dell'ambiente;
    \item Capire come comportarsi in modo ottimo.
\end{itemize}
Un primo semplice approccio è quello di utilizzare un riaddattamento di Adaptive Dynamic Programming per il calcolo della policy ottima: \textbf{Active ADP}.
\\ Idea di base:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Images/activeADP.png}
\end{figure}
L'MDP stimato viene risolto con uno dei metodi basati su DP: Value Iteration o Policy Iteration. Nel caso di Value Iteration sarà necessario svolgere un extra step di Policy Extraction.
\\ \textbf{\textcolor{red}{Problema}}: come fare la stima del modello? In passive RL avevamo una policy fissata che guidata la scelta delle azioni, ora la dobbiamo costruire!
\\ Approccio naïve: 
\begin{itemize}
    \item Uso di una policy fissa $\pi_e$ per esplorare l'ambiente.
    \item Costruisco una stima $\langle \hat{P}, \hat{R} \rangle$ come si faceva in passive RL.
    \item Calcolo la policy ottima su $\langle \hat{P}, \hat{R} \rangle$ usando DP.
\end{itemize}
Questo approccio non funziona perchè il modello stimato $\langle    \hat{P}, \hat{R} \rangle$ "rispecchia" la policy $\pi_E$: l'agente ha imparato solo le transizioni e i reward indotte dalla policy (una sola azione per ogni stato!).
\\ Secondo approccio naïve:
\begin{itemize}
    \item Esploro l'ambiente in modo random.
    \item Costruisco una stima $\langle \hat{P}, \hat{R} \rangle$ come si faceva in passive RL.
    \item Calcolo la policy ottima su $\langle \hat{P}, \hat{R} \rangle$ usando DP.
\end{itemize}
Funziona ma può richiedere un tempo molto lungo, tempo nel quale l'agente si comporta in modo random pagando un prezzo. Il gap tra il reward ottenibile con la policy ottima e quello ottenuto con una policy diversa si misura con una quantità detta \textbf{regret}.
\\ Terzo Approccio:
\begin{enumerate}
    \item Scelgo una policy $\pi$ arbitraria.
    \item Esploro l'ambiente usando $\pi$.
    \item Costruisco la stima $\langle \hat{P}, \hat{R} \rangle$ come si faceva in passive RL.
    \item Costruisco la policy ottima $\hat{\pi}^*$ su $\langle \hat{P}, \hat{R} \rangle$ usando DP.
    \item $\pi \leftarrow \hat{\pi}^*$, riparto da 2.
\end{enumerate}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Images/activeRL_approccio.png}
\end{figure}
\textcolor{ForestGreen}{Pro}: lo step 4 di norma converge in fretta se nello step 2 facciamo poche azioni (tipicamente una sola) perchè la stima del modello cambierà di poco e DP verrà inizializzato con la soluzione dell'iterazione precedente (valori o policy).
\\ \textcolor{red}{Contro}: l'agente è \textbf{greedy}, la prossima azione che compie è la migliore rispetto a quanto correntemnte appreso dall'ambiente (è determinata dalla policy $\pi$ che è quella ottima rispetto al modello stimato).
\\ Non è garantita la convergenza alla policy ottima, potrebbe convergere in un minimo locale.
\\ Il problema dell'approccio greedy è ancora una volta legato al trade-off tra exploration e exploitation. Dobbiamo prevedere di fare anche azioni che sono correntemente valutate come subottimali, il loro valore non sta nel reward che promettono, ma ciò che rivelano riguardo all'ambiente.
\\ Approccio $\epsilon$-greedy (dove $\epsilon$ rappresenta una probabilità molto piccola):
\begin{enumerate}
    \item Scelgo una policy $\pi$ arbitraria.
    \item \textbf{Con probabilità $\epsilon$ faccio un'azione random, altrimenti suo $\pi$} $\leftarrow$ Una piccola frazione delle azioni è puramente esplorativa.
    \item Costruisco la stima $\langle \hat{P}, \hat{R} \rangle$ come si faceva in RL.
    \item Calcolo la policy ottima $\hat{\pi}^*$ su $\langle \hat{P}, \hat{R} \rangle$ usando DP.
    \item $\pi \leftarrow \hat{\pi}^*$, riparto da 2.
\end{enumerate}
\textcolor{ForestGreen}{Pro}: convergenza con un tempo sufficientemente grande.
\\ \textcolor{red}{Contro}: continua ad inserire scelte random anche quando la policy ottima è stata trovata.
\\ Prima soluzione: ridurre $\epsilon$ nel tempo, analogamente a quanto fatto con la learning rate in TD learning.
\\ Seconda soluzione: \textbf{\textcolor{red}{funzione di esplorazione}}.
\\ \textbf{Idea}: spostare il trade-off tra exploration ed exploitation dallo step 2 (dove era definito come scelta $\epsilon$-greedy) all'interno dello Step 4, alterando il calcolo della policy ottima.
Sarà la policy stessa a prescrivere le azioni tenendo conto di exploration ed exploitation. Immaginiamo che lo step 4 sia una Value Iteration (vale un discorso analogo per Policy Iteration):
\begin{itemize}
    \item alteriamo il Bellman update in modo che dato un "bonus" di valore agli stati meno visitati: l'agente greedy sarà attratto dal reward e dall'ignoto (due forze potenzialmente in conflitto);
    \item man mano che aumentano le visite a uno stato bonus si riduce: l'agente greedy sarà sempre più attratto dal reward e compirà meno azioni puramente esplorative.
\end{itemize}
L'utilità "alterata" si calcola con una funzione detta di esplorazione: $f(\textcolor{blue}{u}),(\textcolor{red}{n})=u+\frac{V^+}{1+n}$ dove $\textcolor{blue}{u}$ è il vecchio reward scontato nel tempo, $n$ è il numero di visite e $V^+$ è un parametro.
\subsubsection{Funzione di esplorazione}
Il Bellman update della Value Iteration nello step 4 diventa:
\[ V_{k+1}(s) \leftarrow \max_{a} (f(\textcolor{blue}{\sum_{s'}\hat{P}(s'|s,a)[\hat{R}(s,a,s')+\gamma V_k(s')]},\textcolor{red}{\#(s,a)}))\]
\textcolor{blue}{Reward scontato da $s$ con azione $a$ in avanti}
\\ \textcolor{red}{Numero di volte che ho esplorato l'azione $a$ nello stato $s$}
\\ Il principio è lo stesso che abbiamo incontrato con Monte Carlo Tree Search (MCTS). 
\\ Svolgere le azioni che sembrano le migliori dove la definizione di migliore induce due dinamiche:
\begin{itemize}
    \item \textcolor{blue}{Non tralasciare azioni che sembrano "cattive" senza esserne certi, vanno comunque provate un numero sufficiente di volte}.
    \item \textcolor{red}{Non pretendere di calcolare all'esatto il valore di azioni su cui si è ormai confidenti che non siano buone}.
\end{itemize}
\subsection{Learning di Policy Robuste}
Tutti gli approcci model-based (come quello appena visto) si basano sulla risoluzione di un \textcolor{blue}{modello stimato} con un modello definito per il \textcolor{red}{modello esatto}.
\\ Value Iteration e Policy Iteration assumono che il modello su cui lavorano sia quello vero!
\\ Abbiamo già visto come un errore di stima possa risultare in una policy che sbaglia "inconsapevolemente", in certi casi gli sbagli possono risultare grosse penalità. \textbf{Metodo alternativo}: calcolare una policy che sia robusta agli errori di stima del modello.
\\ Accenniamo a due tecniche:
\begin{itemize}
    \item RL Bayesiano
    \item Controllo robusto
\end{itemize}
\subsubsection{RL Bayesiano}
Stima del modello: anzichè usare il metodo a massima verosomiglianza (basato sulle frequenze empiriche delle transizioni osservate) si utilizza un metodo più sofisticato basato su \textbf{filtro di Bayes}.
\\ Il metodo costruisce, a partire dalle osservazioni dell'agente, una \textbf{\textcolor{red}{posterior}} $P$: una distribuzione di probabilità su possibile versioni del modello $m_1, m_2,m_3, \dots$ (anche detta belief).
\\ Data una policy $\pi$, $V_i^\pi$ è la media di tutte le Policy Evalutation di $\pi$ a partire da ogni stato inziale possibile, assumendo che il modello sia $m_i$.
\\ La versione Bayesiana di RL calcola questa policy ottima
\[ \pi^*= \arg\max_{\pi} \sum_i P(m_i)V_i\pi \]
La policy calcolata è ottima considerando ogni possibile versione del modello, pesato per la sua probabilità, quindi in un certo senso robusta.
\subsubsection{Controllo robusto}
Il \textbf{controllo robusto} è un approccio del tutto simile dove però non si considera la posterior. La policy ottima che si va a calcolare è
\[ \pi^*=\arg\max_{\pi}\min_i V_i^\pi \]
\textcolor{blue}{Assunzione worst-case}: la policy calcolata è quella ottima assumendo che il vero modello sia il peggiore tra tutti quelli possibili.
\\ La policy è la soluzione di questo problema di ottimizzazione

\[
\max u \quad \text{s.t. } u \le V_i^{\pi}, \; \forall i
\]
\subsection{Active model-free learning}
Calcolare la policy ottima senza dover mantenere una stima del modello (probabilità di transione e reward). Utilizziamo la stessa idea usata per calcolare la state-value function nel TD learning.
Visto che non abbiamo una policy data da seguire, applichiamo questa idea alla stima della action-value function $Q(s,a)$. Una volta che abbiamo calcolato $Q(s,a)$, la policy ottima si ottiene immediatamente:
\[\pi^*(s)=\arg\max_a Q(s,a)\]
Il metodo basato su temporal difference per la action-value function si chiama Temporal-Difference Q-Learning, o più semplicemente, \textbf{\textcolor{red}{Q-Learning}}.
\subsection{Q-Learning}
Struttura dell'algoritmo eseguito dall'agente:
\begin{enumerate}
    \item Osservo lo stato corrente $s$, inizializzo $Q(s,a)=0, \#(s,a)=0$ per ogni $a$, e scelgo un'azione $a$ in modo arbitrario.
    \item Eseguo $a$ e osservo la transizione: vado nello stato $s'$ e ricevo reward $R(s \xrightarrow[a]{}s')$.
    \item Incremento $\#(s,a)$ e se $s'$ è visitato per la prima volta inizializzo $Q(s',a')=0, \#(s',a')=0$ per ogni $a'$.
    \item \textbf{\textcolor{red}{Update di $Q(s,a)$ sulla base di quanto osservato}}.
    \item \textbf{\textcolor{blue}{Scelgo di nuovo $a$ e riparto da 2}}.
\end{enumerate}
Nello \textbf{\textcolor{red}{Step 4}} faremo come già fatto precedentemente in passive RL: correzione della stima di $Q$ sulla base dell'osservazione $z$.
\\ Nello \textbf{\textcolor{blue}{Step 5}} ritroviamo il trade-off tra esploration ed exploitation che risolveremo usando una funzione di esplorazione $f$.
\subsubsection{Q-Learning: update di $Q(s,a)$}
Osservazione raccolta da $(s \xrightarrow[a]{R}s')$: il reward immediato $R$ più il reward scontato da $s'$ in avanti \textbf{assumendo di usare la policy ottima indotta dalla stima corrente $Q$}.
\[ z=R+\gamma \max_{a'} Q(s',a') \]
\textbf{\textcolor{red}{Update (Step 4)}}:
\[ Q(s,a) \leftarrow Q(s,a) + \textcolor{blue}{\alpha (\#(s,a))}( z - Q(s,a) ) \]
La \textcolor{blue}{learning rate} è la stesa vista per Passive RL: una funzione $\alpha(n)$ decrescente con $n$: più proviamo una coppia stato-azione più impariamo su di essa  e quindi le correzioni vengono attenuate.
\\ Se valgolo le condizioni già viste sulla learning rate ($\sum_{n=1}^\infty \alpha(n) = \infty$, $\sum_{n=1}^\infty \alpha^2(n) < \infty$) l'algoritmo converge alla $Q$ function ottima. \textbf{Vale con qualsiasi esplorazione}
purchè per $t \rightarrow \infty$ ogni coppia stato-azione sia esplorate infinite volte (questa condizione deve essere vera solo nel limite!)
\\ Intuizione: anche se esplorando si eseguono azioni subottimali o random, nella costruzione di $z$ "guardiamo in avanti" assumendo di fare l'azione ottima (operatore max). Nel guardare in avanti quindi ci svincoliamo dalla policy che usiamo per 
esplorare: metodo \textbf{\textcolor{red}{off-policy}}.
\pagebreak
\subsubsection{Q-learning: scelta della prossima azione}
Nello \textbf{\textcolor{blue}{Step 5}} dobbiamo scegliere la prossima azione da compiere. Adottiamo il metodo basato su \textbf{funzione di esplorazione}
\[ a \leftarrow \arg\max_{a} f( Q(s,a), \#(s,a) ) \]
Notiamo un dettaglio dovuto al fatto di essere \textbf{off-policy}: il calcolo di $a$ potrebbe anche essere svolto prima dell'update (step 4) e darebbe lo stesso risultato perchè $Q(s',a')$ non viene aggiornato.
L'aggiornamento riguarda lo stato che abbiamo lasciato ($s$), la scelta della prossima azione invece riguarda lo stato in cui siamo ora ($s'$) in cui i valori di $Q$ sono rimasti invariati. 
\\ In generale: \textcolor{red}{update e scelta della prossima azione sono indipendenti}.
\end{document}